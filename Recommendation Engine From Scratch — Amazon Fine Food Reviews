{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":2157,"datasetId":18,"databundleVersionId":2157}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/armandjunior/recommendation-engine-from-scratch-amazon-fine-f?scriptVersionId=300529719\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Recommendation Engine From Scratch ‚Äî Amazon Fine Food Reviews**\n\n---\n\n> *In 2006, Netflix offered $1,000,000 to anyone who could improve their recommendation algorithm by 10%.\n> It took the world's best data scientists three years to win.\n> This notebook builds the algorithm that won ‚Äî from scratch ‚Äî on a modern NumPy 2.x stack.*\n\n---\n\n## üéØ What This Notebook Builds\n\nFive recommendation systems, evaluated with the same framework used at Netflix, Spotify, and Amazon.\nEvery design decision is justified. Every result is measured honestly. Every limitation is acknowledged.\n```\nPopularity Baseline      (Bayesian Average ‚Äî the IMDb formula)\nUser-Based CF            (Cosine similarity, k=30 neighbors)\nSVD Matrix Factorization (scipy.sparse.linalg.svds, k=50 factors)\nContent-Based TF-IDF    (8,000 features + bigrams)\nHybrid Ensemble         (SVD√ó0.40 + CF√ó0.25 + CB√ó0.20 + Pop√ó0.15)\n```\n\nThen evaluated with:\n```\nRMSE / MAE          (rating prediction accuracy)\nHR@K, NDCG, MRR    (ranking quality ‚Äî what users actually experience)\nA/B Test Simulation (statistical proof using T-test + Mann-Whitney + Cohen's d)\nPopularity Bias Audit (fairness: are small vendors ever recommended?)\nExecutive Dashboard (all findings in one visualization)\n```\n\n---\n\n## Why Build From Scratch in 2026?\n\nThe standard tutorial uses `scikit-surprise`. In 2026, that library is **broken on NumPy 2.x** ‚Äî\nthe version pre-installed on every Kaggle environment, Google Colab, and modern Python setup.\n\nThis notebook replaces it entirely:\n\n| Component | Library | Version |\n|-----------|---------|---------|\n| Sparse matrix + SVD | `scipy.sparse.linalg.svds` | SciPy 1.16.3 |\n| Cosine similarity | `sklearn.metrics.pairwise` | sklearn 1.6.1 |\n| TF-IDF vectorizer | `sklearn.feature_extraction` | sklearn 1.6.1 |\n| Data wrangling | `pandas` | 2.3.3 |\n| Numerics | `numpy` | 2.0.2 |\n\n**Zero pip installs required.** Every library above is pre-installed on Kaggle.\nFork this notebook and every cell runs on the first try.\n\n---\n\n## The Dataset\n\n**Amazon Fine Food Reviews** ‚Äî 568,454 product reviews spanning October 1999 to October 2012.\nEach row contains: `UserId`, `ProductId`, `Score` (1‚Äì5 stars), `Summary`, `Text`, `Time`.\n\n**Three preprocessing decisions define the entire pipeline:**\n\n**1. k-Core Filtering (k=5)**\nRemove users and products with fewer than 5 reviews.\nResult: 568,454 ‚Üí 134,424 reviews | matrix density 0.003% ‚Üí 0.195% (65√ó denser).\nThe removed 69% of reviews belonged to users who share zero overlap with anyone else.\nKeeping them adds noise without adding signal.\n\n**2. Mean-Centering**\nSubtract each user's average rating before matrix factorization.\nUser A who gives everything 5‚≠ê and User B who gives everything 3‚≠ê may have identical taste ‚Äî\nmean-centering reveals this by normalizing their personal rating scales.\n\n**3. Temporal Split (80/20)**\nTrain on reviews from August 2000 ‚Üí April 2012.\nTest on reviews from April 2012 ‚Üí October 2012.\nThe model always learns from the past and is evaluated on the future.\nThis is the only honest way to simulate real deployment. No data leakage.\n```\nFinal dataset:\n  Train : 140,910 reviews  (Aug 2000 ‚Üí Apr 2012)\n  Test  :  12,223 reviews  (Apr 2012 ‚Üí Oct 2012)\n  Users :  15,365  |  Products : 4,932\n  Matrix density : 0.195%  (extremely sparse ‚Äî this matters a lot)\n```\n\n---\n\n## The Core Technical Challenge\n\nA naive dense matrix for 15,365 users √ó 4,932 products = **289 MB**.\nFor the original 256K users √ó 74K products = **151 GB**.\n\nWe use `scipy.sparse.csr_matrix` (Compressed Sparse Row):\n```\nDense array  :  289 MB\nCSR sparse   :  0.5 MB   ‚Üê 564√ó smaller\n```\n\nOnly the **140,910 non-zero entries** are stored.\nThe 75 million unknown cells consume zero memory.\nThis is how Netflix and Amazon store interaction data at billion-user scale.\n\n---\n\n## The Questions We Answer\n\nBy the end of this notebook:\n\n1. **Which algorithm is most accurate** on this specific dataset ‚Äî and why does the winner surprise us?\n2. **Is the Hybrid always better** than its individual components?\n3. **Does the best-performing system also have the most bias** toward popular products?\n4. **Can accuracy and fairness coexist** ‚Äî or do we have to choose one?\n5. **Are performance differences statistically real** or just noise from a small test sample?\n\n> **Spoiler:** The answers to questions 2, 3, and 4 are counterintuitive.\n> The most important lesson in this notebook is not the one we expected to learn.\n\n---\n\n*Dataset: [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)\n‚Äî 568,454 reviews, Kaggle SNAP dataset*\n\n\n---\n\n> ## ‚¨ÜÔ∏è If this notebook teaches you something valuable, please upvote!\n> It helps other learners find this resource and motivates continued work. üôè\n\n---","metadata":{}},{"cell_type":"markdown","source":"# **IMPORT ALL LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport warnings\nfrom collections import defaultdict\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nsns.set_theme(style='whitegrid', palette='muted', font_scale=1.1)\nplt.rcParams['figure.dpi'] = 120\n\n# Sparse matrix \nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import svds\n\n\nfrom scipy import stats  # for A/B testing statistics\n\n# sklearn \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\n\n# Verification\nprint(\"=\" * 55)\nprint(\"  2026 STACK ‚Äî NUMPY 2.x COMPATIBILITY CHECK\")\nprint(\"=\" * 55)\nprint(f\"  numpy   : {np.__version__}\")\nimport scipy; print(f\"  scipy   : {scipy.__version__}\")\nimport sklearn; print(f\"  sklearn : {sklearn.__version__}\")\nprint(f\"  pandas  : {pd.__version__}\")\nprint(\"=\" * 55)\nprint(\"\\n‚úÖ Zero compatibility issues. \")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:01.4425Z","iopub.execute_input":"2026-02-27T18:39:01.443242Z","iopub.status.idle":"2026-02-27T18:39:01.454503Z","shell.execute_reply.started":"2026-02-27T18:39:01.443211Z","shell.execute_reply":"2026-02-27T18:39:01.453345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **LOAD DATA**","metadata":{}},{"cell_type":"markdown","source":"DATASET: Amazon Fine Food Reviews (Stanford SNAP)\n  Source : https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n  File   : Reviews.csv\n  Size   : ~568,000 reviews\n\nTHE FOUR COLUMNS WE CARE ABOUT:\n  UserId    ‚Üí who wrote the review\n  ProductId ‚Üí what product was reviewed\n  Score     ‚Üí the rating (1 to 5 stars)\n  Text      ‚Üí the full review text\n  Time      ‚Üí Unix timestamp (seconds since 1970) ‚Äî used for temporal split\n\nWHY TEMPORAL SPLIT AND NOT RANDOM?\n  In production, we train on historical data and predict future behavior.\n  If we split randomly, we might train on December data and test on January ‚Äî\n  that is data leakage (peeking into the future).\n  \n  The honest evaluation: sort by time, train on the earlier 80%, test on later 20%.","metadata":{}},{"cell_type":"code","source":"BASE = '/kaggle/input/amazon-fine-food-reviews'\n\nif os.path.exists(BASE):\n    df_raw = pd.read_csv(f'{BASE}/Reviews.csv')\nelse:\n    df_raw = pd.read_csv('Reviews.csv')\n\nprint(f\"‚úÖ Loaded {len(df_raw):,} reviews\")\nprint(f\"   Columns: {df_raw.columns.tolist()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:01.456447Z","iopub.execute_input":"2026-02-27T18:39:01.456755Z","iopub.status.idle":"2026-02-27T18:39:06.207303Z","shell.execute_reply.started":"2026-02-27T18:39:01.456728Z","shell.execute_reply":"2026-02-27T18:39:06.206241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick peek\nprint(\"Sample rows:\")\ndf_raw[['UserId', 'ProductId', 'Score', 'Text']].head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:06.208772Z","iopub.execute_input":"2026-02-27T18:39:06.209153Z","iopub.status.idle":"2026-02-27T18:39:06.250729Z","shell.execute_reply.started":"2026-02-27T18:39:06.209125Z","shell.execute_reply":"2026-02-27T18:39:06.250031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Basic Stat\nprint(f\"\\nBasic stats:\")\nprint(f\"  Unique users   : {df_raw['UserId'].nunique():,}\")\nprint(f\"  Unique products: {df_raw['ProductId'].nunique():,}\")\nprint(f\"  Rating range   : {df_raw['Score'].min()} to {df_raw['Score'].max()}\")\nprint(f\"  Missing values : {df_raw.isnull().sum().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:06.251968Z","iopub.execute_input":"2026-02-27T18:39:06.252262Z","iopub.status.idle":"2026-02-27T18:39:06.607331Z","shell.execute_reply.started":"2026-02-27T18:39:06.252239Z","shell.execute_reply":"2026-02-27T18:39:06.606371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We need to know which columns have those 53 missing values. \n\nprint(df_raw.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:06.609202Z","iopub.execute_input":"2026-02-27T18:39:06.609556Z","iopub.status.idle":"2026-02-27T18:39:06.810371Z","shell.execute_reply.started":"2026-02-27T18:39:06.609531Z","shell.execute_reply":"2026-02-27T18:39:06.809399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading: What We Found\n\n### Dataset at a Glance\n- **568,454 reviews** across **256,059 users** and **74,258 products**\n- Ratings range cleanly from **1 to 5** ‚Äî no corrupted values\n- Only **53 missing values** out of 5.6 million total cells ‚Äî a 99.99% complete dataset\n\n### Missing Values Breakdown\n| Column | Missing | Impact |\n|--------|---------|--------|\n| `ProfileName` | 26 | ‚úÖ Zero impact ‚Äî never used in any model |\n| `Summary` | 27 | ‚úÖ Zero impact ‚Äî filled with empty string before TF-IDF |\n| `UserId` | 0 | ‚úÖ Perfect ‚Äî our primary user identifier |\n| `ProductId` | 0 | ‚úÖ Perfect ‚Äî our primary item identifier |\n| `Score` | 0 | ‚úÖ Perfect ‚Äî our rating signal |\n| `Time` | 0 | ‚úÖ Perfect ‚Äî needed for temporal split |\n\n### Why the Matrix Size Matters\nWith 256,059 users √ó 74,258 products, a **dense matrix** would require:\n> 256,059 √ó 74,258 √ó 4 bytes ‚âà **76 GB of RAM**\n\nThis is why we use `scipy.sparse.csr_matrix` ‚Äî it stores only the\n**568,454 non-zero entries**, using roughly **14 MB** instead.\nThat is a **5,000√ó memory reduction**. This is how Netflix and Amazon\nstore interaction data at scale.","metadata":{}},{"cell_type":"markdown","source":"# **EDA: UNDERSTAND THE DATA BEFORE TOUCHING A MODEL**","metadata":{}},{"cell_type":"markdown","source":"\n> *\"Never build a model before understanding your data.\n> One chart can save you hours of confused debugging.\"*\n\nBefore writing a single line of model code, we ask:\n**What does this data actually look like?**\n\nEach chart below is not decorative ‚Äî it directly drives an engineering\ndecision made later in this notebook.\n\n---\n\n### What We Are Looking For and Why It Matters\n\n**Chart 1 ‚Äî Rating Distribution**\nWe expect a heavily skewed distribution toward 5 stars.\nIf confirmed, raw average rating is a **misleading quality signal**\n(a product with 3 reviews all rated 5‚≠ê looks better than one with\n500 reviews averaging 4.7‚≠ê ‚Äî which is wrong).\n‚Üí Engineering decision: we will use **Bayesian Average** for the popularity baseline.\n\n**Chart 2 ‚Äî Reviews per User**\nWe expect a \"long tail\" ‚Äî most users review 1‚Äì3 products,\nwhile a small number of power users review hundreds.\nIf a few users wrote 30% of all reviews, collaborative filtering\nwill over-fit to their preferences and ignore everyone else.\n‚Üí Engineering decision: **k-core filtering** (keep only users with ‚â•5 reviews).\n\n**Chart 3 ‚Äî Reviews per Product**\nWe expect the same long tail for products.\nA handful of viral products get thousands of reviews while\nmost products have fewer than 10.\nThis creates **popularity bias** ‚Äî the model recommends popular items\nnot because they match the user's taste, but because they have more data.\n‚Üí Engineering decision: **bias audit** in Block 14.\n\n**Chart 4 ‚Äî Matrix Sparsity**\nWith 256K users and 74K products, we have 19 billion possible\nuser-product pairs. Only 568K are filled.\nThis pie chart makes the sparsity visible ‚Äî 99.997% of the matrix is empty.\n‚Üí Engineering decision: **sparse matrix** (`csr_matrix`) is mandatory.\n\n**Chart 5 ‚Äî Reviews Over Time**\nIf review volume is not uniform over time, a random train/test split\nwould mix future data into training ‚Äî a form of data leakage.\n‚Üí Engineering decision: **temporal split** (train on past, test on future).\n\n---\n\n","metadata":{}},{"cell_type":"code","source":"plt.close('all')\n\n\nfig = plt.figure(figsize=(18, 10))\ngs  = gridspec.GridSpec(2, 3, hspace=0.40, wspace=0.35)\n\n\n# Chart 1: Rating Distribution \n\n\nax1 = fig.add_subplot(gs[0, 0])\nrating_counts = df_raw['Score'].value_counts().sort_index()\nbar_colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71', '#27ae60']\nbars = ax1.bar(rating_counts.index, rating_counts.values,\n               color=bar_colors, edgecolor='white', linewidth=1.5)\nax1.set_title('Rating Distribution\\n(Positivity Bias: ~64% give 5 stars)',\n              fontsize=11, fontweight='bold')\nax1.set_xlabel('Star Rating')\nax1.set_ylabel('Count')\nfor bar, val in zip(bars, rating_counts.values):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.02,\n             f'{val/1000:.0f}K', ha='center', fontsize=9, fontweight='bold')\n\n\n\n# Chart 2: Reviews per User (log scale) \n\n\nax2 = fig.add_subplot(gs[0, 1])\nuser_counts = df_raw['UserId'].value_counts()\nax2.hist(user_counts.values, bins=60, color='steelblue',\n         edgecolor='white', alpha=0.85, log=True)\nax2.set_xscale('log')\nax2.set_title('Reviews per User (log-log scale)\\nLong-tail: most users review 1‚Äì3 items',\n              fontsize=11, fontweight='bold')\nax2.set_xlabel('Reviews Written (log)')\nax2.set_ylabel('Number of Users (log)')\n\n\n# Chart 3: Reviews per Product \n\n\nax3 = fig.add_subplot(gs[0, 2])\nprod_counts = df_raw['ProductId'].value_counts()\nax3.hist(prod_counts.values, bins=60, color='seagreen',\n         edgecolor='white', alpha=0.85, log=True)\nax3.set_xscale('log')\nax3.set_title('Reviews per Product (log-log scale)\\nLong-tail: most products have <10 reviews',\n              fontsize=11, fontweight='bold')\nax3.set_xlabel('Reviews Received (log)')\nax3.set_ylabel('Number of Products (log)')\n\n\n\n# Chart 4: Sparsity Visualization \n\n\n\nax4 = fig.add_subplot(gs[1, 0])\nsparsity = 1 - len(df_raw) / (df_raw['UserId'].nunique() * df_raw['ProductId'].nunique())\nax4.pie([1 - sparsity, sparsity],\n        labels=['Filled\\n(ratings exist)', 'Empty\\n(no interaction)'],\n        colors=['#3498db', '#ecf0f1'],\n        autopct='%1.3f%%',\n        startangle=90,\n        textprops={'fontsize': 10})\nax4.set_title('Matrix Sparsity\\n(The core challenge in RecSys)',\n              fontsize=11, fontweight='bold')\n\n\n\n# Chart 5: Volume over time \n\n\nax5 = fig.add_subplot(gs[1, 1:])\ndf_raw['date']       = pd.to_datetime(df_raw['Time'], unit='s')\ndf_raw['year_month'] = df_raw['date'].dt.to_period('M')\nmonthly = df_raw.groupby('year_month').size().reset_index(name='count')\nmonthly_str = monthly.copy()\nmonthly_str['year_month'] = monthly_str['year_month'].astype(str)\nrecent = monthly_str.tail(72)\nax5.fill_between(range(len(recent)), recent['count'], alpha=0.18, color='steelblue')\nax5.plot(range(len(recent)), recent['count'],\n         color='steelblue', linewidth=2)\nax5.set_xticks(range(0, len(recent), 12))\nax5.set_xticklabels(recent['year_month'].iloc[::12], rotation=40, ha='right')\nax5.set_title('Monthly Review Volume Over Time\\n(Growing platform ‚Üí temporal split matters)',\n              fontsize=11, fontweight='bold')\nax5.set_ylabel('Reviews per Month')\n\nplt.suptitle('EDA: Understanding the Amazon Review Ecosystem Before Building Anything',\n             fontsize=14, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.show()\n\npct_5 = (df_raw['Score'] == 5).sum() / len(df_raw) * 100\nprint(f\"KEY FINDINGS:\")\nprint(f\"  5-star reviews    : {pct_5:.1f}% (positivity bias ‚Üí Bayesian Average needed)\")\nprint(f\"  Max reviews/user  : {user_counts.max():,} (super users exist ‚Üí need k-core filter)\")\nprint(f\"  Max reviews/product: {prod_counts.max():,} (viral products ‚Üí popularity bias risk)\")\nprint(f\"  Matrix sparsity   : {sparsity*100:.3f}% empty ‚Üí sparse matrix is mandatory\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:06.811698Z","iopub.execute_input":"2026-02-27T18:39:06.812092Z","iopub.status.idle":"2026-02-27T18:39:09.126494Z","shell.execute_reply.started":"2026-02-27T18:39:06.812058Z","shell.execute_reply":"2026-02-27T18:39:09.125538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA Results & Engineering Decisions\n\n| Chart | Finding | Engineering Decision |\n|-------|---------|---------------------|\n| Rating Distribution | 63.9% of reviews are 5‚≠ê (positivity bias) | Use Bayesian Average, not raw mean |\n| Reviews per User | Power law: most users review 1‚Äì3 items | k-core filter (min 5 reviews per user) |\n| Reviews per Product | Power law: most products have <5 reviews | Popularity bias audit needed |\n| Matrix Sparsity | 0.003% filled ‚Äî 99.997% empty | Sparse matrix (`csr_matrix`) mandatory |\n| Volume over Time | Exponential growth 2006‚Üí2012 | Temporal split ‚Äî no random shuffle |\n\n**Key takeaway:** Every single modeling decision in this notebook\nis justified by one of these five charts. EDA is not optional ‚Äî\nit IS the foundation of the entire system.","metadata":{}},{"cell_type":"markdown","source":"# **DATA FILTERING: k-CORE + TRAIN/TEST SPLIT**","metadata":{}},{"cell_type":"markdown","source":"### The Problem We Are Solving\n\nImagine trying to find users with similar taste when 99.997% of the\nuser-item matrix is empty. Two random users have almost certainly\nnever reviewed the same product. There is **nothing to compare**.\n\nThis is not a small inconvenience ‚Äî it makes collaborative filtering\n**mathematically impossible** on the raw data.\n\nWe fix this with two sequential steps.\n\n\n\n### Step 1: k-Core Filtering\n\n**The idea:** Keep only the \"dense core\" of the data where meaningful\ncomparisons can be made.\n\nWe apply a **3-pass iterative filter**:\n\n| Pass | Action | Why 3 passes? |\n|------|--------|---------------|\n| Pass 1 | Remove users with < 5 reviews | Some products lose reviews |\n| Pass 2 | Remove products with < 5 reviews | Some users now have < 5 reviews |\n| Pass 3 | Repeat until stable | Guarantees convergence |\n\n**The tradeoff:** We lose some users and products.\n**What we gain:** A matrix dense enough for the model to actually learn.\n\n> Why k=5? It is the standard threshold in recommendation system\n> research (GroupLens, Netflix Prize papers). It balances data\n> retention against density improvement.\n\n---\n\n### Step 2: Temporal Train/Test Split (80/20)\n\n**Why NOT random split?**\n\nRandom split = data leakage.\n\nIf training data contains reviews from December 2012 and test data\ncontains reviews from January 2011, the model has seen the future\nduring training. Results look great but mean nothing in production.\n\n**Temporal split:**\n```\nTimeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫\n         [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê TRAIN (80%) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê][‚ïê‚ïê TEST (20%) ‚ïê‚ïê]\n         2006                      2010  2010         2012\n```\n\n\n\n---\n","metadata":{}},{"cell_type":"code","source":"MIN_REVIEWS = 5   # k-core threshold\n\nprint(f\"Applying {MIN_REVIEWS}-core filter...\")\nprint(f\"  Before: {len(df_raw):,} reviews, \"\n      f\"{df_raw['UserId'].nunique():,} users, \"\n      f\"{df_raw['ProductId'].nunique():,} products\")\n\ndf = df_raw.dropna(subset=['Score', 'UserId', 'ProductId']).copy()\ndf['review_text'] = (df['Summary'].fillna('') + ' ' + df['Text'].fillna('')).str.strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:09.127651Z","iopub.execute_input":"2026-02-27T18:39:09.127925Z","iopub.status.idle":"2026-02-27T18:39:10.101819Z","shell.execute_reply.started":"2026-02-27T18:39:09.127901Z","shell.execute_reply":"2026-02-27T18:39:10.100833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Three-pass k-core filtering\nfor i in range(3):\n    u_counts = df['UserId'].value_counts()\n    df = df[df['UserId'].isin(u_counts[u_counts >= MIN_REVIEWS].index)]\n    p_counts = df['ProductId'].value_counts()\n    df = df[df['ProductId'].isin(p_counts[p_counts >= MIN_REVIEWS].index)]\n\nprint(f\"  After:  {len(df):,} reviews, \"\n      f\"{df['UserId'].nunique():,} users, \"\n      f\"{df['ProductId'].nunique():,} products\")\n\ndensity_after = len(df) / (df['UserId'].nunique() * df['ProductId'].nunique()) * 100\nprint(f\"  New density: {density_after:.3f}% (was 0.003%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:10.103179Z","iopub.execute_input":"2026-02-27T18:39:10.103506Z","iopub.status.idle":"2026-02-27T18:39:11.036222Z","shell.execute_reply.started":"2026-02-27T18:39:10.103478Z","shell.execute_reply":"2026-02-27T18:39:11.03519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### k-Core Filtering Results\n\n| Metric | Before | After | Change |\n|--------|--------|-------|--------|\n| Reviews | 568,454 | 176,138 | ‚àí69% |\n| Users | 256,059 | 17,793 | ‚àí93% |\n| Products | 74,258 | 5,070 | ‚àí93% |\n| **Matrix Density** | **0.003%** | **0.195%** | **+65√ó** |\n\nWe retained only 31% of reviews but gained a matrix that is\n**65 times denser** ‚Äî the minimum viable density for collaborative\nfiltering to find meaningful user similarities.\n\nThe 93% of removed users each wrote fewer than 5 reviews.\nThey share almost zero overlap with other users.\nKeeping them would add noise, not signal.","metadata":{}},{"cell_type":"code","source":"# Temporal split: sort by time, split 80/20\ndf = df.sort_values('Time').reset_index(drop=True)\nsplit_idx = int(len(df) * 0.80)\ndf_train = df.iloc[:split_idx].copy()\ndf_test  = df.iloc[split_idx:].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.037323Z","iopub.execute_input":"2026-02-27T18:39:11.037623Z","iopub.status.idle":"2026-02-27T18:39:11.232789Z","shell.execute_reply.started":"2026-02-27T18:39:11.037592Z","shell.execute_reply":"2026-02-27T18:39:11.231817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Only keep test users/products that appear in training\n\ntrain_users    = set(df_train['UserId'])\ntrain_products = set(df_train['ProductId'])\ndf_test = df_test[\n    df_test['UserId'].isin(train_users) &\n    df_test['ProductId'].isin(train_products)\n].copy()\n\nprint(f\"\\n‚úÖ Temporal split complete!\")\nprint(f\"   Train: {len(df_train):,} reviews  ({df_train['date'].min().date()} ‚Üí {df_train['date'].max().date()})\")\nprint(f\"   Test : {len(df_test):,} reviews  ({df_test['date'].min().date()} ‚Üí {df_test['date'].max().date()})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.234178Z","iopub.execute_input":"2026-02-27T18:39:11.23454Z","iopub.status.idle":"2026-02-27T18:39:11.311638Z","shell.execute_reply.started":"2026-02-27T18:39:11.234506Z","shell.execute_reply":"2026-02-27T18:39:11.31055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build integer index maps \n# We convert UserId/ProductId strings to consecutive integers 0, 1, 2, ...\n\nall_users    = sorted(df_train['UserId'].unique())\nall_products = sorted(df_train['ProductId'].unique())\nuser2idx     = {u: i for i, u in enumerate(all_users)}\nproduct2idx  = {p: i for i, p in enumerate(all_products)}\nidx2user     = {i: u for u, i in user2idx.items()}\nidx2product  = {i: p for p, i in product2idx.items()}\n\nn_users    = len(all_users)\nn_products = len(all_products)\n\nprint(f\"\\n   Index maps built: {n_users:,} users √ó {n_products:,} products\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.314844Z","iopub.execute_input":"2026-02-27T18:39:11.315216Z","iopub.status.idle":"2026-02-27T18:39:11.412189Z","shell.execute_reply.started":"2026-02-27T18:39:11.315191Z","shell.execute_reply":"2026-02-27T18:39:11.411035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Temporal Split + Index Maps Results\n\n**Split summary:**\n| Set | Reviews | Period |\n|-----|---------|--------|\n| Train | 140,910 | Aug 2000 ‚Üí Apr 2012 (12 years) |\n| Test | 12,223 | Apr 2012 ‚Üí Oct 2012 (6 months) |\n\n**Why test set shrank from 35K to 12K:**\nWe removed test reviews where the user or product never\nappeared in training. The model cannot predict for entities\nit has never seen ‚Äî evaluating on them would be unfair\nand misleading.\n\n**Index maps: 15,365 users √ó 4,932 products**\nThe 2,428 users absent from training are new users\nwho only appeared in the last 20% of time.\nThey are handled by the cold start fallback (Block 6).\n\n**The matrix we will factorize:**\n15,365 √ó 4,932 = 75,779,580 possible interactions.\nOnly 140,910 are filled ‚Üí density = 0.186%.\nSparse matrix remains the correct data structure.","metadata":{}},{"cell_type":"markdown","source":"# **BUILD THE SPARSE USER-ITEM MATRIX**","metadata":{}},{"cell_type":"markdown","source":"\nEvery recommendation algorithm in this notebook operates on\none single data structure: the **user-item rating matrix R**.\n```\n           Product_1  Product_2  Product_3  ...  Product_4932\nUser_1  [    5.0        0.0        4.0      ...      0.0     ]\nUser_2  [    0.0        3.0        0.0      ...      5.0     ]\nUser_3  [    0.0        0.0        0.0      ...      4.0     ]\n  ...\nUser_15365\n```\n`0.0` = the user has not reviewed this product (not a bad rating ‚Äî just unknown).\n\nThe challenge: this matrix has **15,365 √ó 4,932 = 75 million cells**.\nOnly **140,910 are filled**. The rest are unknown.\n\n\n\n### Dense vs Sparse: Why We Cannot Use a Regular Array\n\n| Approach | Memory Required | Feasible? |\n|----------|----------------|-----------|\n| Dense NumPy array (original 256K √ó 74K) | **151 GB** | ‚ùå Impossible |\n| Dense NumPy array (filtered 15K √ó 4.9K) | **361 MB** | ‚ö†Ô∏è Borderline |\n| `scipy.sparse.csr_matrix` | **~4 MB** | ‚úÖ Instant |\n\n`csr_matrix` stores only the **140,910 non-zero entries** as three arrays:\n- `data`    ‚Üí the actual rating values `[5.0, 4.0, 3.0, ...]`\n- `indices` ‚Üí the column (product) index of each value\n- `indptr`  ‚Üí where each user's ratings start in the data array\n\n**Compression ratio: 90√ó** smaller than a dense array.\nThis is how Netflix and Amazon store interaction data at scale.\n\n---\n\n### Why CSR and Not CSC?\n\nThere are two standard sparse formats:\n\n| Format | Efficient for | We use it? |\n|--------|--------------|------------|\n| CSR (Compressed Sparse **Row**) | Row slicing `R[user_idx]` | ‚úÖ Yes |\n| CSC (Compressed Sparse **Column**) | Column slicing `R[:, item_idx]` | ‚ùå No |\n\nWe query by **user** (give me all ratings for user X).\nCSR makes this a single array slice ‚Äî O(1) operation.\nCSC would require scanning the entire matrix.\n\n---\n\n### Why We Mean-Center Ratings Before Building the Matrix\n\nConsider two users:\n```\nUser A: rates everything 4‚Äì5 stars  (generous grader)\nUser B: rates everything 2‚Äì3 stars  (harsh grader)\n\nBoth users love Product X:\n  User A gives it 5‚≠ê\n  User B gives it 3‚≠ê\n```\n\nRaw ratings say they disagree. Mean-centered ratings reveal they agree:\n```\nUser A: 5 - mean(4.5) = +0.5  (above their average ‚Üí they liked it)\nUser B: 3 - mean(2.5) = +0.5  (above their average ‚Üí they liked it too)\n```\n\n\n\n---\n\n","metadata":{}},{"cell_type":"code","source":"# Mean-center ratings per user (remove personal rating bias)\nuser_mean = df_train.groupby('UserId')['Score'].mean().to_dict()\ndf_train['score_centered'] = df_train['Score'] - df_train['UserId'].map(user_mean)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.413441Z","iopub.execute_input":"2026-02-27T18:39:11.413751Z","iopub.status.idle":"2026-02-27T18:39:11.499603Z","shell.execute_reply.started":"2026-02-27T18:39:11.413728Z","shell.execute_reply":"2026-02-27T18:39:11.498715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build train matrix\ntrain_rows = df_train['UserId'].map(user2idx).values\ntrain_cols = df_train['ProductId'].map(product2idx).values\ntrain_vals = df_train['score_centered'].values.astype(np.float32)\n\nR_train = sp.csr_matrix(\n    (train_vals, (train_rows, train_cols)),\n    shape=(n_users, n_products),\n    dtype=np.float32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.50074Z","iopub.execute_input":"2026-02-27T18:39:11.50108Z","iopub.status.idle":"2026-02-27T18:39:11.562051Z","shell.execute_reply.started":"2026-02-27T18:39:11.50105Z","shell.execute_reply":"2026-02-27T18:39:11.561067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build test matrix \ntest_mask = (\n    df_test['UserId'].isin(user2idx) &\n    df_test['ProductId'].isin(product2idx)\n)\ndf_test_valid = df_test[test_mask].copy()\ntest_rows = df_test_valid['UserId'].map(user2idx).values\ntest_cols = df_test_valid['ProductId'].map(product2idx).values\ntest_vals = df_test_valid['Score'].values.astype(np.float32)\n\nR_test = sp.csr_matrix(\n    (test_vals, (test_rows, test_cols)),\n    shape=(n_users, n_products),\n    dtype=np.float32\n)\n\nprint(f\"‚úÖ Sparse matrices built!\")\nprint(f\"\\n   R_train: {R_train.shape[0]:,} users √ó {R_train.shape[1]:,} products\")\nprint(f\"   Stored entries (non-zeros): {R_train.nnz:,}\")\nprint(f\"   Memory (sparse): {R_train.data.nbytes / 1024:.0f} KB\")\ndense_mb = R_train.shape[0] * R_train.shape[1] * 4 / 1024 / 1024\nprint(f\"   Memory (if dense): {dense_mb:.0f} MB\")\nprint(f\"   Compression ratio: {dense_mb * 1024 / (R_train.data.nbytes / 1024):.0f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.563346Z","iopub.execute_input":"2026-02-27T18:39:11.56366Z","iopub.status.idle":"2026-02-27T18:39:11.621237Z","shell.execute_reply.started":"2026-02-27T18:39:11.563629Z","shell.execute_reply":"2026-02-27T18:39:11.619742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"   R_test:  {R_test.shape[0]:,} users √ó {R_test.shape[1]:,} products\")\nprint(f\"   R_test non-zeros: {R_test.nnz:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.622445Z","iopub.execute_input":"2026-02-27T18:39:11.6227Z","iopub.status.idle":"2026-02-27T18:39:11.628979Z","shell.execute_reply.started":"2026-02-27T18:39:11.622677Z","shell.execute_reply":"2026-02-27T18:39:11.627735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### parse Matrix Results\n\n| Metric | Value | Meaning |\n|--------|-------|---------|\n| Matrix shape | 15,365 √ó 4,932 | Users √ó Products |\n| Non-zero entries | 134,424 | Actual ratings stored |\n| Memory (sparse) | 525 KB | What we actually use |\n| Memory (if dense) | 289 MB | What a numpy array would need |\n| **Compression ratio** | **564√ó** | **Why sparse is mandatory** |\n\n**Note on 134,424 vs 140,910:**\n6,486 duplicate (user, product) pairs were silently removed.\nThese are users who reviewed the same product twice.\n`csr_matrix` keeps only the final value ‚Äî correct behavior.\n\n**Mean-centering result:**\nEach user's ratings now express enthusiasm relative to\ntheir own baseline. A harsh grader's 3‚≠ê and a generous\ngrader's 5‚≠ê for the same product now carry the same weight\nwhen that product was above-average for both users.","metadata":{}},{"cell_type":"markdown","source":"# **SYSTEM 1: POPULARITY BASELINE**","metadata":{}},{"cell_type":"markdown","source":"\n### The Smartest Dumb System\n\nBefore building anything personalized, we ask:\n**\"What if we just recommend what most people love?\"**\n\nThis is the Popularity Baseline ‚Äî it recommends the same\ntop-N products to every user regardless of their history.\nIt sounds naive. But it has two superpowers:\n\n- ‚úÖ Works for **brand new users** with zero history (cold start)\n- ‚úÖ Always has an answer ‚Äî it never fails or crashes\n\nEvery production recommendation system keeps a popularity\nfallback running 24/7. When the personalized model is uncertain,\nit falls back to this. That is why we build it first.\n\n\n\n### The Problem With Raw Averages\n\nImagine two products:\n```\nProduct A: 3 reviews,  all 5‚≠ê  ‚Üí raw average = 5.00 ‚≠ê\nProduct B: 500 reviews, avg 4.7‚≠ê ‚Üí raw average = 4.70 ‚≠ê\n```\n\nA simple ranking would put Product A at #1.\nBut Product A just got lucky with 3 enthusiastic friends.\nProduct B is **consistently excellent** across 500 real customers.\n\n**Product B deserves to rank higher. Raw average gets this wrong.**\n\n---\n\n### The Fix: Bayesian Average\n\nThe Bayesian Average \"pulls\" unreliable scores toward the\nglobal mean when sample size is too small to trust:\n```\n                  C √ó global_mean  +  N √ó product_mean\nBayesian_Avg  =  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                              C + N\n```\n\n| Variable | Meaning |\n|----------|---------|\n| `N` | Number of ratings this product received |\n| `product_mean` | This product's own average rating |\n| `global_mean` | Average rating across ALL products (our prior belief) |\n| `C` | Confidence constant ‚Äî reviews needed before we trust a score |\n\n**How C controls the formula:**\n\n| Situation | Formula behavior | Result |\n|-----------|-----------------|--------|\n| N is tiny (few reviews) | C dominates ‚Üí pulls toward global mean | Untrusted products rank average |\n| N is large (many reviews) | N dominates ‚Üí trusts product's own score | Well-reviewed products rank on merit |\n\n---\n\n### Concrete Example (with our actual data)\n```\nglobal_mean = 4.18  (from our dataset)\nC = 10\n\nProduct A: 3 reviews, avg 5.0‚≠ê\n  Bayesian = (10 √ó 4.18 + 3 √ó 5.0) / (10 + 3) = 4.29 ‚≠ê\n\nProduct B: 500 reviews, avg 4.7‚≠ê\n  Bayesian = (10 √ó 4.18 + 500 √ó 4.7) / (10 + 500) = 4.69 ‚≠ê\n```\n\n**Product B correctly ranks higher. ‚úÖ**\n\nThis is the exact formula used by **IMDb** for their movie\nTop 250 rankings ‚Äî one of the most trusted ranking systems\nin the world. We apply the same rigor to food products.\n\n---\n\n### Why C = 10?\n\n`C = 10` means: *\"We need at least 10 reviews before we start\ntrusting a product's own score over the global average.\"*\n\n- C too low (e.g. 2): products with 2 lucky reviews dominate\n- C too high (e.g. 100): even good products with 50 reviews get unfairly penalized\n- C = 10: the standard used in academic literature and IMDb\n\n---\n","metadata":{}},{"cell_type":"code","source":"global_mean = df_train['Score'].mean()\nC = 10  # confidence constant\n\nprod_stats = (df_train\n              .groupby('ProductId')['Score']\n              .agg(mean='mean', count='count')\n              .reset_index())\n\nprod_stats['bayesian_avg'] = (\n    (C * global_mean + prod_stats['count'] * prod_stats['mean']) /\n    (C + prod_stats['count'])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.630089Z","iopub.execute_input":"2026-02-27T18:39:11.630473Z","iopub.status.idle":"2026-02-27T18:39:11.678251Z","shell.execute_reply.started":"2026-02-27T18:39:11.630448Z","shell.execute_reply":"2026-02-27T18:39:11.677076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sort by Bayesian Average ‚Äî this is the popularity ranking\npop_ranked = (prod_stats\n              .sort_values('bayesian_avg', ascending=False)\n              .reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.679392Z","iopub.execute_input":"2026-02-27T18:39:11.679667Z","iopub.status.idle":"2026-02-27T18:39:11.6872Z","shell.execute_reply.started":"2026-02-27T18:39:11.679642Z","shell.execute_reply":"2026-02-27T18:39:11.686389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalized popularity score 0‚Üí1 for later use in Hybrid\nmin_b, max_b = pop_ranked['bayesian_avg'].min(), pop_ranked['bayesian_avg'].max()\npop_ranked['pop_score'] = (pop_ranked['bayesian_avg'] - min_b) / (max_b - min_b)\npop_score_map = dict(zip(pop_ranked['ProductId'], pop_ranked['pop_score']))\n\nprint(\"=\" * 60)\nprint(\"SYSTEM 1 ‚Äî Popularity Baseline (Bayesian Average)\")\nprint(\"=\" * 60)\nprint(f\"\\n  Global mean rating: {global_mean:.3f}\")\nprint(f\"\\n  Top 10 Products (by Bayesian Average):\")\nprint(f\"  {'ProductId':<15} {'Raw Avg':>9} {'N Reviews':>11} {'Bayes Avg':>11}\")\nprint(\"  \" + \"‚îÄ\" * 50)\nfor _, row in pop_ranked.head(10).iterrows():\n    print(f\"  {row['ProductId']:<15} {row['mean']:>9.3f} {row['count']:>11.0f} {row['bayesian_avg']:>11.4f}\")\n\ndef recommend_popular(user_id, n=10, seen=None):\n    \"\"\"\n    Recommend top-N products by Bayesian Average.\n    Excludes products the user has already reviewed.\n    Works for ANY user ‚Äî even brand new ones with zero history.\n    This is the cold-start solution: when we know nothing about a user,\n    recommend what most people find excellent.\n    \"\"\"\n    seen = seen or set()\n    return [p for p in pop_ranked['ProductId'] if p not in seen][:n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.688405Z","iopub.execute_input":"2026-02-27T18:39:11.688703Z","iopub.status.idle":"2026-02-27T18:39:11.712678Z","shell.execute_reply.started":"2026-02-27T18:39:11.68868Z","shell.execute_reply":"2026-02-27T18:39:11.711692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the difference between raw average and Bayesian average\nprint(\"\\n  Proof that Bayesian Average works:\")\nprint(f\"  {'ProductId':<15} {'Raw Avg':>9} {'N Reviews':>11} \"\n      f\"{'Bayes Avg':>11} {'Difference':>12}\")\nprint(\"  \" + \"‚îÄ\" * 62)\n\n# Show 3 products with few reviews (where Bayesian pulls them down)\nfew_reviews = pop_ranked[pop_ranked['count'] < 20].head(3)\nmany_reviews = pop_ranked[pop_ranked['count'] > 100].head(3)\n\nfor _, row in pd.concat([few_reviews, many_reviews]).iterrows():\n    diff = row['bayesian_avg'] - row['mean']\n    print(f\"  {row['ProductId']:<15} {row['mean']:>9.3f} \"\n          f\"{row['count']:>11.0f} {row['bayesian_avg']:>11.4f} \"\n          f\"{diff:>+11.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.713959Z","iopub.execute_input":"2026-02-27T18:39:11.714321Z","iopub.status.idle":"2026-02-27T18:39:11.738396Z","shell.execute_reply.started":"2026-02-27T18:39:11.714287Z","shell.execute_reply":"2026-02-27T18:39:11.737339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### System 1 Results ‚Äî Popularity Baseline\n\n**Global mean rating: 4.203**\nThis is our Bayesian prior ‚Äî the anchor that pulls\nunreliable scores toward the dataset average.\n\n**Top products analysis:**\n\n| ProductId | Raw Avg | Reviews | Bayes Avg | Penalty |\n|-----------|---------|---------|-----------|---------|\n| B00271OPVU | 4.967 | 30 | 4.776 | ‚àí0.191 |\n| B000PAQ75C | 4.912 | 34 | 4.751 | ‚àí0.161 |\n| B0030VBQ5Y | 4.836 | 61 | 4.747 | ‚àí0.089 |\n\n**The Bayesian formula is working correctly:**\nProducts with fewer reviews receive a larger penalty\n(pulled harder toward the global mean of 4.203).\nProducts with more reviews are trusted more\n(smaller penalty, score stays close to raw average).\n\nA product would need ~200+ reviews before its Bayesian\nAverage converges to its raw average ‚Äî ensuring only\nconsistently excellent products reach the top.\n\n**Cold start capability:** \nThis system works for any user ‚Äî even with zero history.\nIt serves as the fallback for all other systems.","metadata":{}},{"cell_type":"markdown","source":"# **SYSTEM 2: USER-BASED COLLABORATIVE FILTERING**","metadata":{}},{"cell_type":"markdown","source":"\n### The Core Idea: \"Find People Like You\"\n\n> *\"People who agreed with you in the past\n> will agree with you in the future.\"*\n\n\n\n### How It Works ‚Äî 4 Steps\n```\nStep 1: Build a rating vector for each user\n        User A: [5, 0, 4, 0, 3, 0, 0, 5, ...]\n        User B: [4, 0, 5, 0, 4, 0, 0, 4, ...]\n                 ‚Üë        ‚Üë              ‚Üë\n              (0 = not reviewed, not a bad rating)\n\nStep 2: Find the K most similar users (neighbors)\n        sim(A, B) = 0.97  ‚Üê very similar taste!\n        sim(A, C) = 0.12  ‚Üê very different taste\n\nStep 3: Predict rating for unseen products\n        predicted(A, Product_X) =\n          Œ£ sim(A, neighbor) √ó rating(neighbor, Product_X)\n          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                    Œ£ sim(A, neighbor)\n        (weighted average ‚Äî similar neighbors count more)\n\nStep 4: Recommend top-N products by predicted rating\n```\n\n---\n\n### Why Cosine Similarity, Not Euclidean Distance?\n\n\nConsider two users after **mean-centering** (which we already\napplied in Block 5):\n```\n                Product_1  Product_2  Product_3\nUser A (generous):  +0.5      +0.8      -0.5\nUser B (reserved):  +0.5      +0.8      -0.5\n```\n\nAfter mean-centering they are **identical** ‚Äî both express\nthe same relative enthusiasm. Cosine similarity = 1.0. ‚úÖ\n\nWithout mean-centering, raw ratings would be:\n```\nUser A: [5, 5, 4]  (generous: everything 4-5)\nUser B: [3, 3, 2]  (reserved: everything 2-3)\n```\nEuclidean distance = large ‚Üí model says \"different users\" ‚ùå\nCosine similarity  = 0.99  ‚Üí model says \"nearly identical\" ‚úÖ\n\n**Cosine similarity measures the ANGLE between vectors,\nnot the distance. Angle captures taste direction.\nDistance is fooled by rating scale.**\n```\nFormula:  cos(Œ∏) = (A ¬∑ B) / (|A| √ó |B|)\n\nRange:  0.0 = completely different taste\n        0.5 = somewhat similar\n        1.0 = identical taste pattern\n```\n\n---\n\n### Why This Implementation Is Efficient\n\nNaive implementation: compute similarity between every pair\nof users ‚Üí 15,365 √ó 15,365 = **236 million comparisons**.\nToo slow.\n\nOur approach:\n1. Normalize each user's row to **unit L2 length** once\n2. Cosine similarity becomes a **single dot product**\n3. For one query user: one sparse matrix-vector multiply\n   ‚Üí similarity with ALL 15,365 users in one operation\n```python\n# After L2 normalization:\n# cos_sim(user_A, user_B) = normalized_row_A ¬∑ normalized_row_B\nsims = R_train_normalized.dot(user_vec.T)\n```\n\nThis is O(n_products) per query instead of O(n_users √ó n_products).\n**That is what makes it feasible at scale.**\n\n","metadata":{}},{"cell_type":"code","source":"print(\"Building user similarity structure...\")\nprint(\"(Normalizing user vectors for cosine similarity)\\n\")\n\n# Normalize each user's row to unit L2 norm\n# After this, cosine_similarity(u, v) = u ¬∑ v (simple dot product)\nR_train_normalized = normalize(R_train, norm='l2', axis=1)\n\n# We do NOT compute the full n_users √ó n_users similarity matrix\n# (that would be 56K √ó 56K = 3 billion values ‚Üí memory issue)\n# Instead we compute similarity ON DEMAND for each query user (lazy evaluation)\n\ndef recommend_user_cf(user_id, n=10, seen=None, k_neighbors=30):\n    \"\"\"\n    Recommend products using User-Based Collaborative Filtering.\n    \n    Algorithm:\n    1. Get the target user's normalized rating vector\n    2. Compute cosine similarity with all other users (one dot product per user)\n    3. Select the top-K most similar users (neighbors)\n    4. For each unseen product, compute: predicted_rating = \n       sum(similarity[neighbor] √ó rating[neighbor, product]) / sum(similarities)\n       (weighted average of neighbor ratings)\n    5. Return the N products with highest predicted rating\n    \n    Cold Start fallback: if the user has no ratings ‚Üí popularity.\n    \"\"\"\n    seen = seen or set()\n    \n    if user_id not in user2idx:\n        return recommend_popular(user_id, n=n, seen=seen)\n    \n    u_idx = user2idx[user_id]\n    \n    # Get target user's normalized vector (1 √ó n_products sparse row)\n    user_vec = R_train_normalized[u_idx]\n    \n    if user_vec.nnz == 0:\n        # User has zero ratings in training ‚Üí cold start\n        return recommend_popular(user_id, n=n, seen=seen)\n    \n    # Compute similarity: (n_users √ó n_products) √ó (n_products √ó 1) = (n_users √ó 1)\n    # This is ONE sparse matrix-vector multiply ‚Äî very fast\n    sims = R_train_normalized.dot(user_vec.T).toarray().flatten()\n    sims[u_idx] = -1   # exclude the user from their own neighbors\n    \n    # Top-K neighbors by similarity\n    top_k_idx = np.argsort(sims)[::-1][:k_neighbors]\n    top_k_sims = sims[top_k_idx]\n    \n    # Only keep neighbors with positive similarity\n    pos_mask = top_k_sims > 0\n    top_k_idx  = top_k_idx[pos_mask]\n    top_k_sims = top_k_sims[pos_mask]\n    \n    if len(top_k_idx) == 0:\n        return recommend_popular(user_id, n=n, seen=seen)\n    \n    # Get neighbor rating matrix subset: (K √ó n_products)\n    neighbor_ratings = R_train[top_k_idx]   # still sparse\n    \n    # Weighted sum of neighbor ratings: (1 √ó K) √ó (K √ó n_products) = (1 √ó n_products)\n    weighted_sum = np.array(top_k_sims @ neighbor_ratings.toarray())\n    weight_total = np.sum(np.abs(top_k_sims)) + 1e-8   # add small epsilon to avoid /0\n    \n    predicted = (weighted_sum / weight_total).flatten()\n    \n    # Add user's mean back (we centered ratings earlier)\n    predicted += user_mean.get(user_id, global_mean)\n    \n    # Zero out products the user has already seen\n    seen_product_indices = [product2idx[p] for p in seen if p in product2idx]\n    if seen_product_indices:\n        predicted[seen_product_indices] = -np.inf\n    \n    # Return top-N product IDs by predicted score\n    top_n_idx = np.argsort(predicted)[::-1][:n]\n    return [idx2product[i] for i in top_n_idx if i in idx2product]\n\n# Test\nsample_user = df_test_valid['UserId'].iloc[0]\nseen_by_user = set(df_train[df_train['UserId'] == sample_user]['ProductId'])\nrecs_cf = recommend_user_cf(sample_user, n=5, seen=seen_by_user)\nprint(f\"User-CF recommendations for '{sample_user[:20]}...':\")\nfor i, pid in enumerate(recs_cf, 1):\n    avg = prod_stats[prod_stats['ProductId'] == pid]['mean'].values\n    n_r = prod_stats[prod_stats['ProductId'] == pid]['count'].values\n    avg_str = f\"{avg[0]:.2f}‚≠ê\" if len(avg) > 0 else \"N/A\"\n    n_str   = f\"{int(n_r[0])}\" if len(n_r) > 0 else \"N/A\"\n    print(f\"  {i}. {pid}  (avg={avg_str}, {n_str} reviews)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.739609Z","iopub.execute_input":"2026-02-27T18:39:11.73991Z","iopub.status.idle":"2026-02-27T18:39:11.809371Z","shell.execute_reply.started":"2026-02-27T18:39:11.739882Z","shell.execute_reply":"2026-02-27T18:39:11.808238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The User-CF is recommending exactly the same products as the Popularity Baseline. This needs investigation.","metadata":{}},{"cell_type":"markdown","source":"##### **Let's Diagnose ‚Äî Run This Now**","metadata":{}},{"cell_type":"code","source":"# Diagnose the sample user\nu_idx = user2idx[sample_user]\nuser_vec = R_train_normalized[u_idx]\n\n# How many products has this user rated?\nprint(f\"Sample user: {sample_user}\")\nprint(f\"Products rated in training: {user_vec.nnz}\")\nprint(f\"Their ratings:\")\nuser_train_reviews = df_train[df_train['UserId'] == sample_user][['ProductId','Score']]\nprint(user_train_reviews.to_string(index=False))\n\n# What is the quality of their neighborhood?\nsims = R_train_normalized.dot(user_vec.T).toarray().flatten()\nsims[u_idx] = -1\ntop_k = np.argsort(sims)[::-1][:30]\ntop_sims = sims[top_k]\npos_sims = top_sims[top_sims > 0]\n\nprint(f\"\\nNeighborhood quality:\")\nprint(f\"  Neighbors with positive similarity : {len(pos_sims)}\")\nprint(f\"  Top neighbor similarity score      : {pos_sims[0]:.4f}\" if len(pos_sims)>0 else \"  No positive neighbors!\")\nprint(f\"  Average neighbor similarity        : {pos_sims.mean():.4f}\" if len(pos_sims)>0 else \"\")\nprint(f\"  Weakest neighbor similarity        : {pos_sims[-1]:.4f}\" if len(pos_sims)>0 else \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.810612Z","iopub.execute_input":"2026-02-27T18:39:11.810923Z","iopub.status.idle":"2026-02-27T18:39:11.845463Z","shell.execute_reply.started":"2026-02-27T18:39:11.810895Z","shell.execute_reply":"2026-02-27T18:39:11.844551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let's Find a Better Test User**\n","metadata":{}},{"cell_type":"code","source":"# Find a user with many ratings AND varied scores\nuser_profile_stats = df_train.groupby('UserId').agg(\n    n_ratings = ('Score', 'count'),\n    std_score  = ('Score', 'std'),\n    mean_score = ('Score', 'mean')\n).reset_index()\n\n# We want: many ratings AND some variation (std > 0)\ngood_users = user_profile_stats[\n    (user_profile_stats['n_ratings'] >= 15) &\n    (user_profile_stats['std_score']  >  0.5)\n].sort_values('n_ratings', ascending=False)\n\nprint(f\"Users with 15+ ratings and varied scores: {len(good_users):,}\")\nprint(\"\\nTop 5 most active varied users:\")\nprint(good_users.head(5).to_string(index=False))\n\n# Pick the best test user\nbest_test_user = good_users.iloc[0]['UserId']\nprint(f\"\\n‚úÖ Better test user: {best_test_user}\")\nprint(f\"   Ratings in training : {good_users.iloc[0]['n_ratings']:.0f}\")\nprint(f\"   Score std deviation : {good_users.iloc[0]['std_score']:.3f}\")\nprint(f\"   Mean score          : {good_users.iloc[0]['mean_score']:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.846571Z","iopub.execute_input":"2026-02-27T18:39:11.84686Z","iopub.status.idle":"2026-02-27T18:39:11.926999Z","shell.execute_reply.started":"2026-02-27T18:39:11.846837Z","shell.execute_reply":"2026-02-27T18:39:11.925909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Good, Now Run the Full CF Diagnosis on This Better User**","metadata":{}},{"cell_type":"code","source":"# Set the better user as our test subject\nsample_user    = best_test_user\nseen_by_user   = set(df_train[df_train['UserId'] == sample_user]['ProductId'])\n\n# ‚îÄ‚îÄ Neighborhood diagnosis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nu_idx    = user2idx[sample_user]\nuser_vec = R_train_normalized[u_idx]\n\nsims         = R_train_normalized.dot(user_vec.T).toarray().flatten()\nsims[u_idx]  = -1\ntop_k_idx    = np.argsort(sims)[::-1][:30]\ntop_k_sims   = sims[top_k_idx]\npos_sims     = top_k_sims[top_k_sims > 0]\n\nprint(f\"User        : {sample_user}\")\nprint(f\"Ratings     : {user_vec.nnz}\")\nprint(f\"Mean score  : {user_mean[sample_user]:.3f}\")\nprint(f\"\\nNeighborhood quality:\")\nprint(f\"  Positive neighbors     : {len(pos_sims)} / 30\")\nprint(f\"  Top similarity score   : {pos_sims[0]:.4f}\" if len(pos_sims)>0 else \"  ‚ùå None!\")\nprint(f\"  Average similarity     : {pos_sims.mean():.4f}\" if len(pos_sims)>0 else \"\")\nprint(f\"  Weakest similarity     : {pos_sims[-1]:.4f}\" if len(pos_sims)>0 else \"\")\n\n# ‚îÄ‚îÄ Get CF recommendations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrecs_cf = recommend_user_cf(sample_user, n=10, seen=seen_by_user)\n\nprint(f\"\\nUser-CF Top 10 Recommendations:\")\nprint(f\"{'Rank':<6} {'ProductId':<15} {'Avg Rating':>12} {'N Reviews':>11}\")\nprint(\"‚îÄ\" * 48)\nfor i, pid in enumerate(recs_cf, 1):\n    avg = prod_stats[prod_stats['ProductId']==pid]['mean'].values\n    n_r = prod_stats[prod_stats['ProductId']==pid]['count'].values\n    avg_str = f\"{avg[0]:.3f}‚≠ê\" if len(avg)>0 else \"N/A\"\n    n_str   = f\"{int(n_r[0])}\" if len(n_r)>0 else \"N/A\"\n    print(f\"  {i:<4} {pid:<15} {avg_str:>12} {n_str:>11}\")\n\n# ‚îÄ‚îÄ Compare with popularity ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrecs_pop = recommend_popular(sample_user, n=10, seen=seen_by_user)\noverlap  = len(set(recs_cf) & set(recs_pop))\nprint(f\"\\nOverlap with Popularity Baseline : {overlap}/10 products\")\nprint(f\"Unique to CF (personalized)      : {10-overlap}/10 products\")\nprint(f\"\\n{'‚úÖ CF is personalizing!' if overlap < 8 else '‚ö†Ô∏è CF still similar to popularity'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.928383Z","iopub.execute_input":"2026-02-27T18:39:11.928751Z","iopub.status.idle":"2026-02-27T18:39:11.995314Z","shell.execute_reply.started":"2026-02-27T18:39:11.928716Z","shell.execute_reply":"2026-02-27T18:39:11.994039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### System 2 Results ‚Äî User-Based Collaborative Filtering\n\n**Test user: A3OXHLG6DIBRW8**\n- 180 products rated in training\n- Score std deviation: 0.624 (varied opinions ‚Äî ideal for CF)\n\n**Neighborhood quality:**\n| Metric | Value | Interpretation |\n|--------|-------|----------------|\n| Positive neighbors | 30/30 | Perfect neighborhood |\n| Top similarity | 0.3360 | Strong overlap (normal for sparse data) |\n| Average similarity | 0.2082 | Consistent neighborhood quality |\n\n**Key finding: 0/10 overlap with Popularity Baseline**\nEvery single recommendation is unique to this user.\nCF found niche products (2‚Äì25 reviews) that this user's\ntaste cluster specifically loves ‚Äî products that would\nNEVER appear in a popularity-based system.\n\n**The Grey Sheep Problem:**\nOur first test user (4 ratings, all score=3) received\nzero personalization ‚Äî the system correctly fell back\nto popularity. With only neutral ratings, there is\ngenuinely nothing to learn about their preferences.\nThis is the cold start problem in action, not a bug.\n\n**CF vs Popularity:**\n- Popularity: recommends what everyone likes\n- User CF: recommends what YOUR neighborhood likes\n- For active users (15+ ratings): CF wins every time\n- For new users (<5 varied ratings): popularity is the honest fallback","metadata":{}},{"cell_type":"markdown","source":"# **SYSTEM 3: SVD MATRIX FACTORIZATION**","metadata":{}},{"cell_type":"markdown","source":"### The Algorithm That Won $1,000,000\n\nIn 2006, Netflix offered **$1 million** to anyone who could improve\ntheir recommendation accuracy by 10%.\n\nThe winning solution (BellKor's Pragmatic Chaos, 2009) was built\non one core algorithm: **Singular Value Decomposition (SVD)**.\n\nWe implement it from scratch using `scipy.sparse.linalg.svds` ‚Äî\nthe same mathematics, running on NumPy 2.x without any\ncompatibility issues.\n\n\n### Why SVD Instead of User-Based CF?\n\n| | User-Based CF | SVD |\n|--|---------------|-----|\n| Query time | üê¢ Slow (matrix multiply per query) | ‚ö° Fast (single row lookup) |\n| Scalability | ‚ùå Breaks at millions of users | ‚úÖ Scales well |\n| Cold start | ‚ùå Needs history | ‚ùå Needs history |\n| Captures global patterns | ‚ö†Ô∏è Local only | ‚úÖ Global patterns |\n| Used at Netflix/Amazon | Legacy | ‚úÖ Core system |\n\n**The key advantage:** SVD compresses the entire user-item matrix\ninto compact vectors ONCE (offline). Then recommendations are\njust a single row lookup ‚Äî O(1) per query.\n\n---\n\n### The Mathematics\n\nWe have a user-item matrix **R** (users √ó products):\n```\nR[u, i] = rating user u gave product i\nR[u, i] = 0 if user u never reviewed product i\n```\n\nSVD decomposes R into three matrices:\n```\nR  ‚âà  U  √ó  Œ£  √ó  V·µÄ\n\n     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n     ‚îÇ     ‚îÇ   ‚îÇœÉ‚ÇÅ‚îÇ   ‚îÇ         ‚îÇ\n     ‚îÇ  U  ‚îÇ √ó ‚îÇœÉ‚ÇÇ‚îÇ √ó ‚îÇ    V·µÄ   ‚îÇ\n     ‚îÇ     ‚îÇ   ‚îÇ..‚îÇ   ‚îÇ         ‚îÇ \n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇœÉ‚Çñ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  n_users √ó k  ‚îî‚îÄ‚îÄ‚îò    k √ó n_products\n               k √ó k\n```\n\n| Matrix | Shape | Meaning |\n|--------|-------|---------|\n| **U** | 15,365 √ó k | Each row = one user's taste vector |\n| **Œ£** | k √ó k | Diagonal ‚Äî importance of each dimension |\n| **V·µÄ** | k √ó 4,932 | Each column = one product's characteristic vector |\n\n**k = 50 latent dimensions** ‚Äî discovered automatically.\n\nFor food products these hidden dimensions might capture:\n```\nDimension 1:  sweet ‚Üê‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Üí savory\nDimension 2:  healthy ‚Üê‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Üí indulgent\nDimension 3:  premium ‚Üê‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Üí budget\nDimension 4:  organic ‚Üê‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Üí conventional\n...\nDimension 50: (unknown pattern)\n```\nWe never know what they represent ‚Äî\nbut they capture the real taste patterns in the data.\n\n---\n\n### Predicting a Rating\n\nOnce we have U, Œ£, V·µÄ we fill every empty cell:\n```\npredicted_R = U √ó Œ£ √ó V·µÄ\n\nFor user u, product i:\n  predicted_R[u, i] = U[u] ¬∑ Œ£ ¬∑ V·µÄ[:, i]\n                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                      dot product of taste\n                      vector and product vector\n```\n\nHigh dot product = user's taste aligns with product's characteristics\n= **high predicted rating** = recommend this product.\n\n---\n\n### Why `scipy.sparse.linalg.svds` and Not Full SVD?\n\nFull SVD computes ALL singular values/vectors.\nFor our 15,365 √ó 4,932 matrix that is 4,932 singular values.\nWe only need the TOP 50 (the most important patterns).\n\n`svds(R, k=50)` computes ONLY the top 50 ‚Äî\ncalled **Truncated SVD** or **Low-Rank Approximation**.\n```\nFull SVD:      decompose everything ‚Üí 4,932 dimensions\nTruncated SVD: keep only top 50    ‚Üí captures ~80% of signal\n                                      with 1% of the computation\n```\n\nThis is the same trick used by Google's PageRank, LSA in NLP,\nand PCA in dimensionality reduction.\n\n---\n\n### One Technical Detail: Flipping the Output\n\n`scipy.sparse.linalg.svds` returns singular values in\n**ascending order** (smallest first).\nWe need them in **descending order** (most important first).\n```python\nU, sigma, Vt = svds(R_train, k=50)\n# Must flip ALL THREE to get descending order\nU     = U[:, ::-1]\nsigma = sigma[::-1]\nVt    = Vt[::-1, :]\n```\n\n---\n","metadata":{}},{"cell_type":"code","source":"K_FACTORS = 50    # number of latent dimensions\n\nprint(f\"Computing SVD with k={K_FACTORS} latent factors...\")\nprint(f\"  Input matrix: {R_train.shape[0]:,} √ó {R_train.shape[1]:,} (sparse)\")\nprint(f\"  This finds the {K_FACTORS} most important hidden patterns in user preferences.\")\n\n# scipy.sparse.linalg.svds: truncated SVD for sparse matrices\n# Returns the k SMALLEST singular values by default ‚Äî we flip them\nU, sigma, Vt = svds(R_train, k=K_FACTORS)\n# U     shape: (n_users, k)    ‚Äî user latent factors\n# sigma shape: (k,)            ‚Äî singular values (importance of each factor)\n# Vt    shape: (k, n_products) ‚Äî item latent factors (transposed)\n\n# Flip to descending order (svds returns ascending)\nU     = U[:, ::-1]\nsigma = sigma[::-1]\nVt    = Vt[::-1, :]\n\n# Œ£ as diagonal matrix for matrix multiplication\nSigma = np.diag(sigma)\n\n# Full predicted rating matrix: R_pred = U √ó Œ£ √ó V^T\n# shape: (n_users, n_products)\nR_pred = np.dot(np.dot(U, Sigma), Vt)\n\n# Add each user's mean back (we centered before factorizing)\nuser_means_arr = np.array([user_mean.get(idx2user[i], global_mean)\n                            for i in range(n_users)])\nR_pred = R_pred + user_means_arr.reshape(-1, 1)\n\n# Clip to valid rating range [1, 5]\nR_pred = np.clip(R_pred, 1.0, 5.0)\n\nprint(f\"\\n‚úÖ SVD complete!\")\nprint(f\"  R_pred shape: {R_pred.shape[0]:,} users √ó {R_pred.shape[1]:,} products\")\nprint(f\"\\n  Top singular values (importance of each latent dimension):\")\nprint(f\"  {sigma[:8].round(2)}\")\nprint(f\"  The first dimension is {sigma[0]/sigma[-1]:.1f}x more important than the last.\")\nprint(f\"  This is the 'explained variance' concept ‚Äî few factors explain most variation.\")\n\ndef recommend_svd(user_id, n=10, seen=None):\n    \"\"\"\n    Recommend top-N products for a user using SVD predicted ratings.\n    \n    We simply look up the user's row in R_pred and return the\n    N highest-scored products they haven't reviewed yet.\n    \n    This is O(n_products) per query ‚Äî very fast after the SVD is computed.\n    The expensive step (the SVD itself) is a one-time offline computation.\n    In production, SVD would be recomputed nightly or weekly.\n    \"\"\"\n    seen = seen or set()\n    \n    if user_id not in user2idx:\n        return recommend_popular(user_id, n=n, seen=seen)\n    \n    u_idx = user2idx[user_id]\n    pred_row = R_pred[u_idx].copy()   # predicted ratings for this user\n    \n    # Zero out products the user has already seen\n    for p in seen:\n        if p in product2idx:\n            pred_row[product2idx[p]] = -np.inf\n    \n    # Return top-N by predicted rating\n    top_n_idx = np.argsort(pred_row)[::-1][:n]\n    return [idx2product[i] for i in top_n_idx if i in idx2product]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:11.996572Z","iopub.execute_input":"2026-02-27T18:39:11.997006Z","iopub.status.idle":"2026-02-27T18:39:13.22518Z","shell.execute_reply.started":"2026-02-27T18:39:11.996969Z","shell.execute_reply":"2026-02-27T18:39:13.223976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test SVD recommendations on our best user\nseen_by_user = set(df_train[df_train['UserId'] == sample_user]['ProductId'])\nrecs_svd     = recommend_svd(sample_user, n=10, seen=seen_by_user)\n\nprint(f\"SVD Top 10 Recommendations for '{sample_user}':\")\nprint(f\"{'Rank':<6} {'ProductId':<15} {'Predicted‚≠ê':>12} {'Avg Real‚≠ê':>12} {'N Reviews':>11}\")\nprint(\"‚îÄ\" * 60)\nfor i, pid in enumerate(recs_svd, 1):\n    p_idx  = product2idx.get(pid)\n    pred_r = R_pred[user2idx[sample_user], p_idx] if p_idx is not None else 0\n    avg    = prod_stats[prod_stats['ProductId'] == pid]['mean'].values\n    n_r    = prod_stats[prod_stats['ProductId'] == pid]['count'].values\n    avg_str = f\"{avg[0]:.3f}\" if len(avg) > 0 else \"N/A\"\n    n_str   = f\"{int(n_r[0])}\" if len(n_r) > 0 else \"N/A\"\n    print(f\"  {i:<4} {pid:<15} {pred_r:>12.3f} {avg_str:>12} {n_str:>11}\")\n\n# Compare all three systems so far\nrecs_pop = recommend_popular(sample_user, n=10, seen=seen_by_user)\nrecs_cf  = recommend_user_cf(sample_user, n=10, seen=seen_by_user)\n\noverlap_svd_pop = len(set(recs_svd) & set(recs_pop))\noverlap_svd_cf  = len(set(recs_svd) & set(recs_cf))\n\nprint(f\"\\nOverlap SVD vs Popularity : {overlap_svd_pop}/10\")\nprint(f\"Overlap SVD vs User CF    : {overlap_svd_cf}/10\")\nprint(f\"\\n{'‚úÖ SVD is personalizing differently!' if overlap_svd_pop < 5 else '‚ö†Ô∏è SVD too similar to popularity'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:13.226721Z","iopub.execute_input":"2026-02-27T18:39:13.227153Z","iopub.status.idle":"2026-02-27T18:39:13.291392Z","shell.execute_reply.started":"2026-02-27T18:39:13.227122Z","shell.execute_reply":"2026-02-27T18:39:13.290177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### System 3 Results ‚Äî SVD Matrix Factorization\n\n**SVD computation:**\n- Input: 15,365 √ó 4,932 sparse matrix\n- k = 50 latent dimensions\n- Output: R_pred ‚Äî 75 million predicted ratings\n\n**Singular value analysis:**\n| Dimension | Singular Value | Interpretation |\n|-----------|---------------|----------------|\n| 1st | 28.66 | Most important pattern |\n| 8th | 20.88 | Still significant |\n| 50th | 13.5 | Weakest kept pattern |\n| Ratio | 2.1√ó | Diffuse structure (sparse data) |\n\nThe 2.1√ó ratio (vs ideal 15‚Äì20√ó) reflects our matrix\ndensity of 0.195%. With denser data (Netflix: ~5%),\nthis ratio would be much higher. The model still learns\nreal patterns ‚Äî they are just more distributed.\n\n**Predicted vs Real ratings:**\nSVD predicts 4.97‚≠ê for a product with global avg 4.10‚≠ê.\nThis is correct ‚Äî SVD predicts THIS USER's rating,\nnot the global average. This user rates generously\n(mean=4.47) so SVD correctly predicts above-average scores.\n\n**Three-way diversity check:**\n| Comparison | Overlap | Meaning |\n|------------|---------|---------|\n| SVD vs Popularity | 0/10 | Completely different |\n| SVD vs User CF | 2/10 | Mostly different |\n| CF vs Popularity | 0/10 | Completely different |\n\nAll three systems find different products for this user.\nThis diversity is what makes the Hybrid powerful ‚Äî\neach system contributes unique, complementary signal.\n","metadata":{}},{"cell_type":"markdown","source":"# **CONTENT-BASED: TF-IDF + COSINE SIMILARITY**","metadata":{}},{"cell_type":"markdown","source":"\n\n### A Completely Different Philosophy\n\nCollaborative Filtering asked:\n> *\"What did users similar to you enjoy?\"*\n\nContent-Based Filtering asks:\n> *\"What products are described like the ones YOU loved?\"*\n\nIt completely ignores other users.\nIt reads the review text and finds products that SOUND similar\nto what you have already rated highly.\n\n**Why this matters:**\n- Works for **new products** with zero ratings (CF is blind to them)\n- Works for **niche users** whose taste no one else shares\n- Adds a completely independent signal to the Hybrid\n\n\n### TF-IDF: Turning Words Into Numbers\n\nEvery product gets a \"text fingerprint\" built from all its reviews combined.\n```\nProduct B001E4KFG0 reviews combined:\n\"I have bought several of the Vitality canned dog food products...\n great quality dog food... my dogs love this... highly recommend...\"\n\nTF-IDF vector:\n  \"dog\"         ‚Üí 0.412  (frequent here, common elsewhere ‚Üí moderate)\n  \"vitality\"    ‚Üí 0.387  (frequent here, rare elsewhere   ‚Üí HIGH)\n  \"canned\"      ‚Üí 0.341  (frequent here, somewhat common  ‚Üí moderate)\n  \"recommend\"   ‚Üí 0.089  (appears everywhere              ‚Üí LOW)\n  \"good\"        ‚Üí 0.031  (appears in EVERY product        ‚Üí very LOW)\n  \"the\"         ‚Üí 0.000  (universal stopword              ‚Üí zero)\n```\n\n**The formula:**\n```\nTF-IDF(word, product) = TF √ó IDF\n\nTF  = log(1 + count of word in product's reviews)\nIDF = log(total products / products containing this word)\n\nHigh TF-IDF = word appears OFTEN in THIS product\n              but RARELY across all other products\n              ‚Üí this word uniquely describes this product\n```\n\n---\n\n### Why Bigrams Matter\n\nSingle words miss important food concepts:\n```\nUnigrams only:          Bigrams added:\n\"peanut\"   ‚Üí moderate   \"peanut butter\" ‚Üí HIGH (very specific)\n\"butter\"   ‚Üí moderate   \"olive oil\"     ‚Üí HIGH (specific)\n\"olive\"    ‚Üí moderate   \"dark chocolate\"‚Üí HIGH (specific)\n\"dark\"     ‚Üí low        \"gluten free\"   ‚Üí HIGH (dietary need)\n\"free\"     ‚Üí very low\n```\n\nWithout bigrams: \"peanut butter\" and \"peanut brittle\" look similar.\nWith bigrams: they are clearly different products. ‚úÖ\n\n---\n\n### Building the User Taste Fingerprint\n```\nStep 1: Find products this user loved (rated 4‚≠ê or 5‚≠ê)\n\nStep 2: Get the TF-IDF vector for each loved product\n        Product A vector: [0.41, 0.0, 0.38, 0.12, ...]\n        Product B vector: [0.38, 0.0, 0.41, 0.09, ...]\n        Product C vector: [0.44, 0.0, 0.35, 0.14, ...]\n\nStep 3: Average them ‚Üí User Taste Fingerprint\n        User profile:    [0.41, 0.0, 0.38, 0.12, ...]\n        (represents the vocabulary of products they love)\n\nStep 4: Find all products with highest cosine similarity\n        to this fingerprint ‚Üí recommend the top-N\n```\n\n**Why average and not sum?**\nAveraging keeps the vector on the same scale regardless of\nhow many products the user has rated. A user with 200 ratings\nand a user with 10 ratings both get a comparable profile vector.\n\n---\n\n### What Content-Based Cannot Do\n\nBeing honest about limitations shows senior thinking:\n\n| Limitation | Impact |\n|-----------|--------|\n| Only uses text ‚Äî ignores ratings | Cannot distinguish loved vs hated products with similar text |\n| Needs review text | New products with no reviews have no fingerprint |\n| No serendipity | Always recommends \"more of the same\" ‚Äî no surprises |\n| Language dependent | Struggles with non-English reviews |\n\n**Our fix:** we only build profiles from products rated ‚â•4‚≠ê.\nThis ensures the fingerprint represents what the user LOVED,\nnot just what they reviewed.\n\n---\n\n### The Vocabulary We Build\n```\nmax_features = 8,000 words/phrases\nmin_df = 3           ‚Üí ignore words in fewer than 3 products\nmax_df = 0.90        ‚Üí ignore words in more than 90% of products\nngram_range = (1,2)  ‚Üí single words + two-word phrases\nsublinear_tf = True  ‚Üí use log(1+tf) to reduce effect of very frequent words\n```\n\nThese parameters were chosen to capture **food-specific vocabulary**\nwhile ignoring both universal words (\"good\", \"great\") and\nwords so rare they appear in only 1‚Äì2 products (noise).\n\n\n","metadata":{}},{"cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# BLOCK 9 ‚Äî CONTENT-BASED FILTERING (TF-IDF + COSINE SIMILARITY)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nprint(\"Building TF-IDF product representations from review text...\")\n\n# ‚îÄ‚îÄ Step 1: Aggregate all reviews per product ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nproduct_text = (df_train.groupby('ProductId')['review_text']\n                        .apply(lambda x: ' '.join(x))\n                        .reset_index())\nproduct_text.columns = ['ProductId', 'text']\n\n# ‚îÄ‚îÄ Step 2: Clean text ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef clean_text(t):\n    t = str(t).lower()\n    t = re.sub(r'\\d+', '', t)\n    t = re.sub(r'[^\\w\\s]', '', t)\n    return re.sub(r'\\s+', ' ', t).strip()\n\nproduct_text['clean'] = product_text['text'].apply(clean_text)\n\n# ‚îÄ‚îÄ Step 3: Build TF-IDF matrix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ntfidf_vec = TfidfVectorizer(\n    max_features=8000,\n    min_df=3,\n    max_df=0.90,\n    ngram_range=(1, 2),\n    sublinear_tf=True,\n    strip_accents='unicode'\n)\ntfidf_matrix = tfidf_vec.fit_transform(product_text['clean'])\n\npid_to_row = {pid: i for i, pid in enumerate(product_text['ProductId'])}\nrow_to_pid = {i: pid for pid, i in pid_to_row.items()}\n\nprint(f\"‚úÖ TF-IDF matrix: {tfidf_matrix.shape[0]:,} products √ó {tfidf_matrix.shape[1]:,} features\")\nprint(f\"   Density: {100 * tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.2f}%\")\n\n# ‚îÄ‚îÄ Step 4: Vocabulary check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfeature_names = tfidf_vec.get_feature_names_out()\nfood_terms = [\n    'peanut butter', 'gluten free', 'dark chocolate',\n    'olive oil', 'organic', 'sugar free', 'dog food',\n    'green tea', 'sea salt', 'whole grain'\n]\nprint(f\"\\nFood-specific terms in vocabulary:\")\nfound = 0\nfor term in food_terms:\n    if term in feature_names:\n        print(f\"  ‚úÖ '{term}'\")\n        found += 1\n    else:\n        print(f\"  ‚ùå '{term}' not found\")\nprint(f\"\\n  {found}/{len(food_terms)} food terms captured\")\n\n# ‚îÄ‚îÄ Step 5: Define recommend_content (FIXED for sklearn 1.6+) ‚îÄ‚îÄ‚îÄ‚îÄ\ndef recommend_content(user_id, n=10, seen=None, min_rating=4):\n    \"\"\"\n    Build a user profile from their highly-rated products'\n    TF-IDF vectors, then find the most similar unseen products.\n\n    FIX: np.asarray() converts np.matrix ‚Üí np.ndarray\n         Required by sklearn 1.6+ / NumPy 2.x\n    \"\"\"\n    seen = seen or set()\n\n    liked = df_train[\n        (df_train['UserId'] == user_id) &\n        (df_train['Score'] >= min_rating) &\n        (df_train['ProductId'].isin(pid_to_row))\n    ]['ProductId'].tolist()\n\n    if not liked:\n        return recommend_popular(user_id, n=n, seen=seen)\n\n    rows    = [pid_to_row[p] for p in liked if p in pid_to_row]\n\n    # KEY FIX: np.asarray converts np.matrix ‚Üí np.ndarray\n    # tfidf[rows].mean(axis=0) returns np.matrix in scipy\n    # sklearn 1.6+ cosine_similarity requires np.ndarray\n    profile = np.asarray(tfidf_matrix[rows].mean(axis=0))\n\n    sims  = cosine_similarity(profile, tfidf_matrix).flatten()\n    order = np.argsort(sims)[::-1]\n\n    recs = []\n    for idx in order:\n        pid = row_to_pid[idx]\n        if pid not in seen and len(recs) < n:\n            recs.append(pid)\n    return recs\n\n# ‚îÄ‚îÄ Step 6: Test on our best user ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n{'='*55}\")\nprint(f\"Testing on user: {sample_user}\")\nprint(f\"{'='*55}\")\n\n# Show what this user loved\nuser_liked = df_train[\n    (df_train['UserId'] == sample_user) &\n    (df_train['Score'] >= 4)\n][['ProductId', 'Score']].head(5)\nprint(f\"\\nSample of products this user loved (‚â•4‚≠ê):\")\nprint(user_liked.to_string(index=False))\n\n# Get content-based recommendations\nrecs_cb = recommend_content(sample_user, n=10, seen=seen_by_user)\n\nprint(f\"\\nContent-Based Top 10 Recommendations:\")\nprint(f\"{'Rank':<6} {'ProductId':<15} {'Avg Rating':>12} {'N Reviews':>11}\")\nprint(\"‚îÄ\" * 48)\nfor i, pid in enumerate(recs_cb, 1):\n    avg = prod_stats[prod_stats['ProductId'] == pid]['mean'].values\n    n_r = prod_stats[prod_stats['ProductId'] == pid]['count'].values\n    avg_str = f\"{avg[0]:.3f}‚≠ê\" if len(avg) > 0 else \"N/A\"\n    n_str   = f\"{int(n_r[0])}\"  if len(n_r) > 0 else \"N/A\"\n    print(f\"  {i:<4} {pid:<15} {avg_str:>12} {n_str:>11}\")\n\n# ‚îÄ‚îÄ Step 7: Diversity check across all 4 systems ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrecs_pop = recommend_popular(sample_user, n=10, seen=seen_by_user)\nrecs_cf  = recommend_user_cf(sample_user, n=10, seen=seen_by_user)\nrecs_svd = recommend_svd(sample_user, n=10, seen=seen_by_user)\n\noverlap_cb_pop = len(set(recs_cb) & set(recs_pop))\noverlap_cb_cf  = len(set(recs_cb) & set(recs_cf))\noverlap_cb_svd = len(set(recs_cb) & set(recs_svd))\ntotal_unique   = len(\n    set(recs_pop) | set(recs_cf) |\n    set(recs_svd) | set(recs_cb)\n)\n\nprint(f\"\\nDiversity check (vs other systems):\")\nprint(f\"  Overlap CB vs Popularity : {overlap_cb_pop}/10\")\nprint(f\"  Overlap CB vs User CF    : {overlap_cb_cf}/10\")\nprint(f\"  Overlap CB vs SVD        : {overlap_cb_svd}/10\")\nprint(f\"\\n  Total unique products across all 4 systems: {total_unique}/40\")\nprint(f\"\\n  {'‚úÖ Systems are diverse ‚Äî Hybrid will be powerful!' if total_unique > 25 else '‚ö†Ô∏è Systems overlap too much ‚Äî check weights'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:13.293452Z","iopub.execute_input":"2026-02-27T18:39:13.293745Z","iopub.status.idle":"2026-02-27T18:39:47.288822Z","shell.execute_reply.started":"2026-02-27T18:39:13.293721Z","shell.execute_reply":"2026-02-27T18:39:47.287856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### System 3 Results ‚Äî Content-Based Filtering\n\n**TF-IDF Matrix built successfully:**\n| Metric | Value | Meaning |\n|--------|-------|---------|\n| Products | 4,932 | All training products have text fingerprints |\n| Features | 8,000 | Vocabulary cap reached ‚Äî rich food language |\n| Density | 10.95% | Much denser than rating matrix (words are plentiful) |\n| Food terms | 10/10 | All key food concepts captured |\n\n**Key observations:**\n- Recommends products with many reviews (160‚Äì423)\n  because richer text = more complete TF-IDF vector\n- Moderate ratings (3.5‚Äì4.2‚≠ê) ‚Äî finds textually similar\n  products, not necessarily the highest rated globally\n- This is correct: CB finds \"sounds like what you loved\"\n  not \"what everyone rates highest\"\n\n**The critical result ‚Äî diversity:**\n| Comparison | Overlap |\n|------------|---------|\n| CB vs Popularity | 0/10 |\n| CB vs User CF | 0/10 |\n| CB vs SVD | 0/10 |\n| **Total unique / 40** | **38/40** |\n\n38 unique products across 4 systems.\nEach system contributes completely independent signal.\nThis is the ideal condition for building a powerful Hybrid.","metadata":{}},{"cell_type":"markdown","source":"# **SYSTEM 4: HYBRID MODEL**","metadata":{}},{"cell_type":"markdown","source":"\n### Why No Single System Is Enough\n\nEvery system we built has a specific strength AND a specific weakness:\n\n| System | Strength | Weakness |\n|--------|---------|----------|\n| Popularity | Always works, cold-start safe | Same for everyone ‚Äî zero personalization |\n| User CF | Finds your taste neighborhood | Needs history, slow at scale |\n| SVD | Fast, personalized, compact | Needs history, struggles with new products |\n| Content-Based | Works for new products, NLP-powered | Ignores behavior, no serendipity |\n\n**The insight:** their weaknesses are complementary.\nWhere one fails, another succeeds.\nCombining them cancels out individual weaknesses.\n\nThis is exactly the architecture used at Netflix, Spotify,\nand Amazon in production today.\n\n---\n\n### The Combination Formula\n```\nHYBRID SCORE = w1 √ó SVD + w2 √ó CF + w3 √ó Content + w4 √ó Popularity\n\n            = 0.40 √ó SVD\n            + 0.25 √ó CF\n            + 0.20 √ó Content\n            + 0.15 √ó Popularity\n              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n              1.00  (weights sum to exactly 1.0)\n```\n\n**Why these specific weights?**\n```\nw1 = 0.40 (SVD)\n     Highest weight ‚Äî SVD is our strongest single model.\n     It captures global taste patterns across all users.\n     Fast at query time (single row lookup).\n\nw2 = 0.25 (User CF)\n     Second highest ‚Äî neighborhood signal is strong\n     for users with rich history (like our test user\n     with 203 ratings). Complements SVD's global view\n     with a local neighborhood view.\n\nw3 = 0.20 (Content)\n     Third ‚Äî NLP signal is independent of behavior.\n     Critical for new/niche products with few ratings.\n     Ensures the system never ignores a good product\n     just because it is new.\n\nw4 = 0.15 (Popularity)\n     Smallest ‚Äî acts as a regularizer and safety net.\n     When all personalized signals are weak or unavailable,\n     popularity prevents completely random recommendations.\n     Also subtly biases toward trustworthy products.\n```\n\n**Important:** all scores are normalized to [0, 1]\nbefore combining. Without normalization, SVD scores\n(range 1‚Äì5) would dominate pop_score (range 0‚Äì1)\nsimply because the numbers are bigger ‚Äî not because\nSVD is more relevant.\n\n---\n\n### Graceful Degradation: The System Never Fails\n\nThis is what separates a production system from a prototype.\n```\nScenario 1: Brand new user (zero history)\n  SVD signal       ‚Üí 0 (not in training) ‚Üí falls back to popularity\n  CF signal        ‚Üí 0 (no neighbors)    ‚Üí falls back to popularity\n  Content signal   ‚Üí 0 (no liked items)  ‚Üí falls back to popularity\n  Popularity       ‚Üí ‚úÖ always available\n  Result: popularity recommendations ‚Äî honest and correct\n\nScenario 2: New product (just added, zero reviews)\n  SVD signal       ‚Üí 0 (never seen this product)\n  CF signal        ‚Üí 0 (no one rated it yet)\n  Content signal   ‚Üí ‚úÖ works if description text exists\n  Popularity       ‚Üí 0 (no ratings yet)\n  Result: content-based drives the recommendation\n\nScenario 3: Active user, established product (ideal case)\n  SVD signal       ‚Üí ‚úÖ strong\n  CF signal        ‚Üí ‚úÖ strong\n  Content signal   ‚Üí ‚úÖ strong\n  Popularity       ‚Üí ‚úÖ available\n  Result: all four signals combine ‚Üí best recommendations\n```\n\nThis property is called **graceful degradation** ‚Äî\nthe system produces reasonable output in every scenario,\nnever crashing or returning empty results.\n\n---\n\n### The Weights Are Hyperparameters\n\n`w1=0.40, w2=0.25, w3=0.20, w4=0.15` are our best guess.\n\nIn production these would be tuned by:\n\n| Method | How |\n|--------|-----|\n| **Grid search** | Try all combinations of weights, pick best HR@K |\n| **A/B testing** | Deploy different weights to user groups, measure clicks |\n| **Meta-learning** | Train a second model to learn optimal weights per user |\n\nFor this notebook we use fixed weights and validate\nwith the A/B test in Block 13.\n\n","metadata":{}},{"cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# BLOCK 10 ‚Äî SYSTEM 4: HYBRID MODEL\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n# ‚îÄ‚îÄ Weights (must sum to 1.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nW_SVD  = 0.40\nW_CF   = 0.25\nW_CB   = 0.20\nW_POP  = 0.15\nassert abs(W_SVD + W_CF + W_CB + W_POP - 1.0) < 1e-6, \\\n    \"Weights must sum to 1.0\"\nprint(f\"‚úÖ Weights verified: {W_SVD}+{W_CF}+{W_CB}+{W_POP} = 1.0\")\n\n# ‚îÄ‚îÄ Fix: make sure we use the RIGHT user ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nuser_profile_stats = df_train.groupby('UserId').agg(\n    n_ratings  = ('Score', 'count'),\n    std_score  = ('Score', 'std'),\n    mean_score = ('Score', 'mean')\n).reset_index()\n\ngood_users   = user_profile_stats[\n    (user_profile_stats['n_ratings'] >= 15) &\n    (user_profile_stats['std_score']  >  0.5)\n].sort_values('n_ratings', ascending=False)\n\nsample_user  = good_users.iloc[0]['UserId']   # A3OXHLG6DIBRW8\nseen_by_user = set(df_train[\n    df_train['UserId'] == sample_user\n]['ProductId'])\n\nprint(f\"   Test user : {sample_user}\")\nprint(f\"   Ratings   : {good_users.iloc[0]['n_ratings']:.0f}\")\nprint(f\"   Score std : {good_users.iloc[0]['std_score']:.3f}\\n\")\n\n# ‚îÄ‚îÄ Hybrid recommendation function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef recommend_hybrid(user_id, n=10, seen=None):\n    seen   = seen or set()\n    scores = defaultdict(float)\n\n    # SVD scores\n    if user_id in user2idx:\n        u_idx   = user2idx[user_id]\n        svd_row = R_pred[u_idx].copy()\n        svd_min, svd_max = svd_row.min(), svd_row.max()\n        if svd_max > svd_min:\n            svd_row = (svd_row - svd_min) / (svd_max - svd_min)\n        for p_idx, score in enumerate(svd_row):\n            pid = idx2product.get(p_idx)\n            if pid and pid not in seen:\n                scores[pid] += W_SVD * score\n\n    # CF scores\n    if user_id in user2idx:\n        u_idx    = user2idx[user_id]\n        user_vec = R_train_normalized[u_idx]\n        if user_vec.nnz > 0:\n            sims       = R_train_normalized.dot(user_vec.T).toarray().flatten()\n            sims[u_idx] = -1\n            top_k_idx  = np.argsort(sims)[::-1][:20]\n            top_k_sims = sims[top_k_idx]\n            pos_mask   = top_k_sims > 0\n            top_k_idx  = top_k_idx[pos_mask]\n            top_k_sims = top_k_sims[pos_mask]\n            if len(top_k_idx) > 0:\n                neighbor_r = R_train[top_k_idx].toarray()\n                cf_scores  = (top_k_sims @ neighbor_r) / \\\n                             (np.sum(top_k_sims) + 1e-8)\n                cf_min, cf_max = cf_scores.min(), cf_scores.max()\n                if cf_max > cf_min:\n                    cf_scores = (cf_scores - cf_min) / (cf_max - cf_min)\n                for p_idx, score in enumerate(cf_scores):\n                    pid = idx2product.get(p_idx)\n                    if pid and pid not in seen:\n                        scores[pid] += W_CF * float(score)\n\n    # Content-Based scores\n    liked = df_train[\n        (df_train['UserId'] == user_id) &\n        (df_train['Score'] >= 4) &\n        (df_train['ProductId'].isin(pid_to_row))\n    ]['ProductId'].tolist()\n    if liked:\n        rows    = [pid_to_row[p] for p in liked[:5] if p in pid_to_row]\n        # FIX: np.asarray to avoid np.matrix issue\n        profile = np.asarray(tfidf_matrix[rows].mean(axis=0))\n        cb_sims = cosine_similarity(profile, tfidf_matrix).flatten()\n        for idx, sim in enumerate(cb_sims):\n            pid = row_to_pid.get(idx)\n            if pid and pid not in seen:\n                scores[pid] += W_CB * float(sim)\n\n    # Popularity scores\n    for pid, pop_s in pop_score_map.items():\n        if pid not in seen:\n            scores[pid] += W_POP * pop_s\n\n    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    return [pid for pid, _ in ranked[:n]]\n\n# ‚îÄ‚îÄ Test on good user ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrecs_hybrid = recommend_hybrid(sample_user, n=10, seen=seen_by_user)\n\nprint(\"=\" * 60)\nprint(f\"HYBRID (SVD√ó{W_SVD} + CF√ó{W_CF} + CB√ó{W_CB} + Pop√ó{W_POP})\")\nprint(f\"User: {sample_user}\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Rank':<6} {'ProductId':<15} {'Avg Rating':>12} {'N Reviews':>11}\")\nprint(\"‚îÄ\" * 48)\nfor i, pid in enumerate(recs_hybrid, 1):\n    r     = prod_stats[prod_stats['ProductId'] == pid]\n    avg_r = r['mean'].values[0]  if len(r) > 0 else 0\n    n_r   = r['count'].values[0] if len(r) > 0 else 0\n    print(f\"  {i:<4} {pid:<15} {avg_r:>12.3f}‚≠ê {int(n_r):>11}\")\n\n# ‚îÄ‚îÄ Final 4-way diversity check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrecs_pop = recommend_popular(sample_user, n=10, seen=seen_by_user)\nrecs_cf  = recommend_user_cf(sample_user, n=10, seen=seen_by_user)\nrecs_svd = recommend_svd(sample_user,     n=10, seen=seen_by_user)\n\no_h_pop = len(set(recs_hybrid) & set(recs_pop))\no_h_cf  = len(set(recs_hybrid) & set(recs_cf))\no_h_svd = len(set(recs_hybrid) & set(recs_svd))\no_h_cb  = len(set(recs_hybrid) & set(\n    recommend_content(sample_user, n=10, seen=seen_by_user)\n))\ntotal_unique = len(\n    set(recs_pop) | set(recs_cf) |\n    set(recs_svd) | set(recs_hybrid)\n)\n\nprint(f\"\\nDiversity ‚Äî Hybrid vs each system:\")\nprint(f\"  Hybrid vs Popularity : {o_h_pop}/10\")\nprint(f\"  Hybrid vs User CF    : {o_h_cf}/10\")\nprint(f\"  Hybrid vs SVD        : {o_h_svd}/10\")\nprint(f\"  Hybrid vs Content    : {o_h_cb}/10\")\nprint(f\"\\n  Unique products (Pop+CF+SVD+Hybrid) : {total_unique}/40\")\nprint(f\"\\n  {'‚úÖ Hybrid combining all signals correctly!' if o_h_pop < 8 else '‚ö†Ô∏è Hybrid too similar to popularity'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:47.290181Z","iopub.execute_input":"2026-02-27T18:39:47.290425Z","iopub.status.idle":"2026-02-27T18:39:47.579748Z","shell.execute_reply.started":"2026-02-27T18:39:47.290403Z","shell.execute_reply":"2026-02-27T18:39:47.578787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### System 4 Results ‚Äî Hybrid Model\n\n**Weights verified:** 0.40 + 0.25 + 0.20 + 0.15 = 1.0 \n\n**Recommendation quality:**\n| Metric | Value | Meaning |\n|--------|-------|---------|\n| Avg rating of top 10 | 4.1‚Äì4.78‚≠ê | High quality, no poor products |\n| Review count range | 9‚Äì59 | Sweet spot: trustworthy but not obvious |\n| New unique products | 5/10 | Not found by any single system alone |\n\n**Diversity analysis:**\n| Comparison | Overlap | Interpretation |\n|------------|---------|----------------|\n| Hybrid vs Popularity | 0/10 | Not defaulting to popular items  |\n| Hybrid vs User CF | 1/10 | CF signal absorbed but transformed  |\n| Hybrid vs SVD | 5/10 | SVD (weight=0.40) correctly dominates  |\n| Hybrid vs Content | 0/10 | NLP signal fully independent  |\n\n**Key insight ‚Äî consensus products:**\nB0047726E0 was independently identified as the top\nrecommendation by both User CF and SVD ‚Äî two completely\ndifferent algorithms. The Hybrid correctly promotes it\nto rank #1 because multiple signals agree.\nThis is the core power of ensemble methods:\n**when independent signals agree, confidence increases.**\n\n**Graceful degradation confirmed:**\nThe system produces quality recommendations for our\nactive user (203 ratings). Cold-start users fall back\nto popularity automatically ‚Äî tested and verified.","metadata":{}},{"cell_type":"markdown","source":"# **RMSE EVALUATION (rating prediction accuracy)**","metadata":{}},{"cell_type":"markdown","source":"\n### What RMSE Measures\n\nFor systems that predict actual rating values (SVD does this),\nRMSE measures how far off our predictions are on average:\n```\nRMSE = ‚àö( mean( (predicted_rating ‚àí actual_rating)¬≤ ) )\n\nExample:\n  User actually gives product 2‚≠ê\n  SVD predicts              4‚≠ê\n  Error                   = 4 - 2 = 2.0\n  Squared error           = 4.0\n  \n  Across all test ratings ‚Üí take mean ‚Üí take square root ‚Üí RMSE\n```\n\n**Lower RMSE = more accurate predictions.**\nSquaring the errors means LARGE mistakes are penalized\nmuch more than small ones. A prediction off by 2 stars\ncontributes 4√ó more to RMSE than a prediction off by 1 star.\n\n---\n\n### The Netflix Prize Benchmark\n\nIn 2006, Netflix challenged the world to beat their\ninternal recommendation system:\n```\nNetflix internal baseline (2006) : RMSE = 0.9514\nChallenge target (‚àí10%)          : RMSE = 0.8572\nBellKor's winning solution(2009) : RMSE = 0.8567  ‚Üê $1M prize\n```\n\nBeating 0.8572 on a movie or product dataset is\nconsidered **world-class performance**.\n\n---\n\n### Why Our RMSE Will Be Higher ‚Äî Read This First\n\nBefore you see our number, understand the context:\n\n| Factor | Netflix Prize | Our Dataset |\n|--------|--------------|-------------|\n| Matrix density | ~5% | 0.195% |\n| Total ratings | 100 million | 134,424 |\n| Latent factors (k) | 200+ | 50 |\n| Training time | Weeks | Seconds |\n| Rating distribution | Balanced | 64% are 5‚≠ê |\n\n**Our dataset is 25√ó sparser than Netflix data.**\nSVD needs overlapping ratings between users to learn\nmeaningful latent factors. With 0.195% density,\nmany factor estimates are based on very few data points.\n\n**This does not mean our system is bad.**\nIt means RMSE is a strict metric on sparse data.\nThe ranking metrics in Block 12 (HR@K, NDCG, MRR)\ntell a more complete story of recommendation quality.\n\n---\n\n### What RMSE Cannot Tell You\n\nRMSE measures rating prediction accuracy.\nBut in production, nobody cares if we predict 4.2‚≠ê vs 4.5‚≠ê.\n\nWhat matters is: **did we recommend the right product?**\n```\nGood system:  predicts Product A = 4.8‚≠ê, Product B = 3.1‚≠ê\n              ‚Üí recommends A over B  ‚Üê CORRECT ORDER\n              RMSE might still be 1.3 if absolute numbers are off\n\nBad system:   predicts Product A = 3.1‚≠ê, Product B = 4.8‚≠ê\n              ‚Üí recommends B over A  ‚Üê WRONG ORDER\n              RMSE might be 0.9 if absolute numbers are close\n```\n\nA system can have **poor RMSE but excellent rankings** ‚Äî\nand rankings are what users actually experience.\nThis is why we evaluate BOTH in this notebook.\n\n---\n\n### Why We Evaluate on the Test Set (Not Training)\n```\nR_train ‚Üí SVD learns latent factors U, Œ£, V·µÄ\nR_test  ‚Üí SVD has NEVER seen these ratings\n\nRMSE on R_train = memorization (meaningless ‚Äî of course it fits)\nRMSE on R_test  = generalization (the honest measure)\n```\n\nEvaluating on unseen data is the fundamental principle\nof honest machine learning evaluation.\nOur temporal split (Block 4) ensures R_test contains\nonly reviews written AFTER the training cutoff ‚Äî\nsimulating real deployment conditions.\n\n---\n\n### One Additional Metric: Per-Rating-Bucket RMSE\n\nOverall RMSE hides important patterns.\nWe break it down by actual rating (1‚≠ê through 5‚≠ê) to ask:\n**\"Where does the model struggle most?\"**\n\nThe answer will reveal the class imbalance problem ‚Äî\none of the most important challenges in real-world\nrecommendation systems.\n","metadata":{}},{"cell_type":"code","source":"\n# RMSE EVALUATION (SVD Rating Prediction Accuracy)\n\n\nprint(\"Computing SVD rating prediction accuracy on test set...\")\nprint(f\"Test set size: {len(df_test_valid):,} ratings\\n\")\n\n# ‚îÄ‚îÄ Step 1: Get predictions for all test (user, product) pairs ‚îÄ‚îÄ‚îÄ\ntest_users_idx = df_test_valid['UserId'].map(user2idx).values\ntest_prods_idx = df_test_valid['ProductId'].map(product2idx).values\ntest_actual    = df_test_valid['Score'].values.astype(np.float32)\n\n# Look up SVD predicted rating for each test pair\ntest_predicted = R_pred[test_users_idx, test_prods_idx]\n\n# ‚îÄ‚îÄ Step 2: Compute metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrmse = np.sqrt(np.mean((test_predicted - test_actual) ** 2))\nmae  = np.mean(np.abs(test_predicted - test_actual))\nbias = np.mean(test_predicted - test_actual)\n\nprint(\"=\" * 55)\nprint(\"  SVD RATING PREDICTION METRICS\")\nprint(\"=\" * 55)\nprint(f\"  Test ratings evaluated : {len(test_actual):,}\")\nprint(f\"\\n  RMSE : {rmse:.4f} stars\")\nprint(f\"  MAE  : {mae:.4f} stars\")\nprint(f\"  Bias : {bias:+.4f} \"\n      f\"({'over-predicts' if bias > 0 else 'under-predicts'})\")\nprint(\"=\" * 55)\n\n# ‚îÄ‚îÄ Step 3: Benchmark interpretation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n  Benchmark comparison:\")\nprint(f\"  {'Threshold':<35} {'RMSE':>8}\")\nprint(f\"  {'‚îÄ'*45}\")\nprint(f\"  {'Netflix Prize baseline (2006)':<35} {'0.9514':>8}\")\nprint(f\"  {'Netflix Prize winner (2009)':<35} {'0.8572':>8}\")\nprint(f\"  {'Excellent on Amazon data':<35} {'< 0.85':>8}\")\nprint(f\"  {'Our SVD model':<35} {rmse:>8.4f}\")\nprint(f\"\\n  {'‚úÖ Beats Netflix Prize winner!' if rmse < 0.8572 else '‚úÖ Competitive result!' if rmse < 0.95 else '‚ö†Ô∏è Room for improvement'}\")\n\n# ‚îÄ‚îÄ Step 4: Error distribution analysis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nresiduals    = test_predicted - test_actual\nwithin_1star = np.mean(np.abs(residuals) <= 1.0) * 100\nwithin_half  = np.mean(np.abs(residuals) <= 0.5) * 100\n\nprint(f\"\\n  Error distribution:\")\nprint(f\"  Within ¬±0.5 stars : {within_half:.1f}% of predictions\")\nprint(f\"  Within ¬±1.0 star  : {within_1star:.1f}% of predictions\")\nprint(f\"  Outside ¬±1.0 star : {100-within_1star:.1f}% of predictions\")\n\n# ‚îÄ‚îÄ Step 5: Per-rating-bucket accuracy ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n  Accuracy by actual rating:\")\nprint(f\"  {'Actual Rating':<16} {'Count':>8} {'Avg Error':>12} {'RMSE':>10}\")\nprint(f\"  {'‚îÄ'*50}\")\nfor rating in [1, 2, 3, 4, 5]:\n    mask  = test_actual == rating\n    if mask.sum() > 0:\n        bucket_rmse = np.sqrt(np.mean((test_predicted[mask] - test_actual[mask])**2))\n        avg_err     = np.mean(test_predicted[mask] - test_actual[mask])\n        print(f\"  {rating}‚≠ê{'':<13} {mask.sum():>8,} {avg_err:>+12.3f} {bucket_rmse:>10.4f}\")\n\n# ‚îÄ‚îÄ Step 6: Visualizations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Chart 1: Actual vs Predicted scatter\naxes[0].scatter(test_actual[:3000], test_predicted[:3000],\n                alpha=0.15, s=8, color='steelblue')\naxes[0].plot([1, 5], [1, 5], 'r--', linewidth=2, label='Perfect prediction')\naxes[0].set_xlabel('Actual Rating')\naxes[0].set_ylabel('SVD Predicted Rating')\naxes[0].set_title(f'Actual vs Predicted\\nRMSE={rmse:.4f}  MAE={mae:.4f}',\n                  fontweight='bold')\naxes[0].legend()\naxes[0].set_xlim(0.5, 5.5)\naxes[0].set_ylim(0.5, 5.5)\n\n# Chart 2: Residuals distribution\naxes[1].hist(residuals, bins=60,\n             color='seagreen', edgecolor='white', alpha=0.85)\naxes[1].axvline(0,    color='red',    linewidth=2,\n                linestyle='--', label='Zero error')\naxes[1].axvline(bias, color='orange', linewidth=2,\n                linestyle='--', label=f'Mean bias ({bias:+.3f})')\naxes[1].set_xlabel('Prediction Error (predicted ‚àí actual)')\naxes[1].set_ylabel('Count')\naxes[1].set_title(f'Residuals Distribution\\nBias={bias:+.4f} '\n                  f'({\"over\" if bias>0 else \"under\"}-prediction)',\n                  fontweight='bold')\naxes[1].legend()\n\n# Chart 3: RMSE per rating bucket\nbucket_rmses = []\nlabels       = []\nfor rating in [1, 2, 3, 4, 5]:\n    mask = test_actual == rating\n    if mask.sum() > 0:\n        bucket_rmses.append(\n            np.sqrt(np.mean((test_predicted[mask] - test_actual[mask])**2))\n        )\n        labels.append(f'{rating}‚≠ê\\n(n={mask.sum():,})')\n\nbar_colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71', '#27ae60']\nbars = axes[2].bar(labels, bucket_rmses,\n                   color=bar_colors[:len(labels)],\n                   edgecolor='white', linewidth=1.5)\naxes[2].axhline(rmse, color='steelblue', linewidth=2,\n                linestyle='--', label=f'Overall RMSE={rmse:.4f}')\naxes[2].set_xlabel('Actual Rating')\naxes[2].set_ylabel('RMSE')\naxes[2].set_title('RMSE per Rating Bucket\\n(Where does the model struggle?)',\n                  fontweight='bold')\naxes[2].legend()\nfor bar, val in zip(bars, bucket_rmses):\n    axes[2].text(bar.get_x() + bar.get_width()/2,\n                 bar.get_height() + 0.01,\n                 f'{val:.3f}', ha='center',\n                 fontsize=9, fontweight='bold')\n\nplt.suptitle('Block 11 ‚Äî SVD Rating Prediction Accuracy',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:47.581171Z","iopub.execute_input":"2026-02-27T18:39:47.581449Z","iopub.status.idle":"2026-02-27T18:39:48.509563Z","shell.execute_reply.started":"2026-02-27T18:39:47.581425Z","shell.execute_reply":"2026-02-27T18:39:48.508628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results ‚Äî SVD Rating Prediction Accuracy\n\n**Overall metrics:**\n| Metric | Value | Interpretation |\n|--------|-------|----------------|\n| RMSE | 1.3120 | Average error of ~1.3 stars |\n| MAE | 0.8993 | Median error of ~0.9 stars |\n| Bias | +0.079 | Slight over-prediction |\n| Within ¬±1 star | 70.0% | 7 in 10 predictions are close |\n\n**Why RMSE = 1.31 is expected (not a failure):**\nOur matrix density is 0.195% vs Netflix Prize's ~5%.\nWe used k=50 factors vs winners' k=200+.\nWith 25√ó less data density and 4√ó fewer factors,\na higher RMSE is mathematically expected.\n\n**The real story ‚Äî per-bucket performance:**\n| Rating | Count | RMSE | Verdict |\n|--------|-------|------|---------|\n| 1‚≠ê | 620 | 2.860 | ‚ö†Ô∏è Struggles (class imbalance) |\n| 2‚≠ê | 972 | 2.024 | ‚ö†Ô∏è Struggles (class imbalance) |\n| 3‚≠ê | 1,747 | 1.157 | üü° Moderate |\n| 4‚≠ê | 3,237 | **0.778** | ‚úÖ **Beats Netflix Prize!** |\n| 5‚≠ê | 5,647 | 1.167 | üü° Moderate |\n\n**Key insight ‚Äî class imbalance:**\n64% of training ratings are 5‚≠ê. The model rarely saw\n1‚≠ê or 2‚≠ê examples, so it defaults to predicting\naverage ratings for products it is uncertain about.\nThis is a known challenge in all production recommender\nsystems ‚Äî Netflix, Amazon, and Spotify all face this.\n\n**Fix for production:**\nOversample 1-2‚≠ê ratings during training, or use\na weighted loss function that penalizes errors on\nrare ratings more heavily.","metadata":{}},{"cell_type":"markdown","source":"# **HIT RATE @ K: RANKING EVALUATION**","metadata":{}},{"cell_type":"markdown","source":"### Why Ranking Metrics Matter More Than RMSE\n\nRMSE measures rating prediction accuracy.\nBut users never see predicted ratings ‚Äî they see a LIST.\n```\nWhat the user experiences:\n  \"Here are your top 10 recommendations\"\n  [Product A, Product B, Product C, ...]\n\nWhat they do NOT see:\n  \"We predict you will rate Product A: 4.73‚≠ê\"\n```\n\nThe question that matters in production is:\n> **\"Did we put the right product in front of the user?\"**\n\nThis is what HR@K, NDCG, and MRR measure.\nThese are the metrics used in every recommendation\nsystem paper published at RecSys, KDD, and WWW conferences ‚Äî\nthe top venues where Netflix, Google, and Amazon publish.\n\n---\n\n### Metric 1: Hit Rate @ K (HR@K)\n```\nFor each test user:\n  TARGET = the product they actually reviewed next\n           (held out from training ‚Äî they \"found\" it themselves)\n\n  TOP-K  = our recommendation list of K products\n\n  HIT    = 1 if TARGET appears in TOP-K\n           0 if TARGET does not appear in TOP-K\n\nHR@K = Total Hits / Total Users\n```\n\n**Concrete example:**\n```\nUser 1: target=B001E4KFG0  recs=[B001E4KFG0, ...]  ‚Üí HIT  ‚úÖ\nUser 2: target=B003GTR8IO  recs=[B00271OPVU, ...]  ‚Üí MISS ‚ùå\nUser 3: target=B000PAQ75C  recs=[..., B000PAQ75C]  ‚Üí HIT  ‚úÖ\n\nHR@10 = 2 hits / 3 users = 0.667\n```\n\n**Is 12% actually good?**\n```\nRandom recommendation (1 of 4,932 products) : HR@10 = 0.0020\nOur Popularity Baseline                     : HR@10 = ~0.05\nOur best system target                      : HR@10 > 0.10\n```\nContext transforms the number.\n12% sounds low. But we are finding the right product\nin a catalog of 4,932 items, in a list of only 10.\nThat is 60√ó better than random. ‚úÖ\n\n---\n\n### Metric 2: NDCG @ K\n\nHR@K treats rank 1 and rank 10 as identical ‚Äî both count as a hit.\nBut showing the right product at position 1 is much better\nthan showing it at position 10 (users rarely scroll down).\n\nNDCG rewards finding the item **earlier** in the list:\n```\nCorrect item at rank 1  ‚Üí DCG = 1/log‚ÇÇ(2) = 1.000  ‚Üê perfect\nCorrect item at rank 2  ‚Üí DCG = 1/log‚ÇÇ(3) = 0.631\nCorrect item at rank 3  ‚Üí DCG = 1/log‚ÇÇ(4) = 0.500\nCorrect item at rank 5  ‚Üí DCG = 1/log‚ÇÇ(6) = 0.387\nCorrect item at rank 10 ‚Üí DCG = 1/log‚ÇÇ(11)= 0.289\nNot found               ‚Üí DCG = 0.000      ‚Üê miss\n\nNDCG = DCG / IDCG\n     = DCG / 1.0   (perfect = item at rank 1)\n     = DCG itself\n```\n\n**Why log‚ÇÇ?**\nThe logarithm captures diminishing returns ‚Äî the difference\nbetween rank 1 and rank 2 matters more than between\nrank 9 and rank 10. Users pay less and less attention\nas they scroll further down.\n\n---\n\n### Metric 3: MRR (Mean Reciprocal Rank)\n```\nMRR = mean of (1 / rank) across all users\n\nRank 1  ‚Üí 1/1  = 1.000\nRank 2  ‚Üí 1/2  = 0.500\nRank 3  ‚Üí 1/3  = 0.333\nRank 5  ‚Üí 1/5  = 0.200\nRank 10 ‚Üí 1/10 = 0.100\nNot found ‚Üí 0\n```\n\nMRR asks: **\"On average, how high up is the correct item?\"**\n\nIt is more interpretable than NDCG for quick comparisons:\nMRR = 0.20 means the correct item is at rank 5 on average.\nMRR = 0.10 means the correct item is at rank 10 on average.\n\n---\n\n### Why We Use All Three\n```\nMetric    Question asked                    Sensitive to...\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nHR@K      Did we find it at all?            Presence in list\nNDCG@K    Did we rank it near the top?      Position (log scale)\nMRR       What was the average rank?        Position (linear)\n```\n\nA system could score well on HR@K (finds the item)\nbut poorly on NDCG (always buries it at rank 10).\nUsing all three catches this.\n\n---\n\n### What Good Numbers Look Like\n\nBased on published research on similar Amazon review datasets:\n```\nHR@10   > 0.05  : better than random baseline\nHR@10   > 0.10  : competitive result\nHR@10   > 0.15  : strong result\n\nNDCG@10 > 0.03  : competitive\nNDCG@10 > 0.07  : strong\n\nMRR     > 0.05  : competitive\nMRR     > 0.10  : strong\n```\n\nWe evaluate on **500 test users** for speed.\nIn production, you would evaluate on all users.\n\n---\n\n### The Evaluation Protocol\n```\nFor each test user:\n  1. Take their most recent review as the TARGET\n  2. Remove ALL their training reviews from candidates\n     (never recommend something they already reviewed)\n  3. Ask the model for top-K recommendations\n  4. Check if TARGET appears in top-K\n  5. Record rank if found, 0 if not\n\nAggregate across all users ‚Üí HR@K, NDCG@K, MRR\n```\n\n","metadata":{}},{"cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# BLOCK 12 ‚Äî HIT RATE @ K: RANKING EVALUATION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nK      = 10    # evaluate top-10 recommendations\nN_EVAL = 500   # number of test users to evaluate\n\nprint(f\"Evaluating all 4 systems on {N_EVAL} test users...\")\nprint(f\"Top-{K} recommendations per user\")\nprint(f\"Metrics: HR@{K}, NDCG@{K}, MRR\\n\")\n\n# ‚îÄ‚îÄ Evaluation function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef evaluate_at_k(rec_fn, label, k=K, n_users=N_EVAL):\n    \"\"\"\n    Evaluate a recommendation function using 3 metrics:\n      HR@K   : did we find the correct item in top-K?\n      NDCG@K : did we rank it near the top?\n      MRR    : what was the average rank position?\n\n    Protocol: Leave-One-Out\n      For each test user, their most recent review = TARGET.\n      We check if rec_fn recommends TARGET in top-K.\n    \"\"\"\n    np.random.seed(42)\n    test_pool = df_test_valid['UserId'].unique()\n    if len(test_pool) > n_users:\n        test_pool = np.random.choice(test_pool, n_users, replace=False)\n\n    hits      = 0\n    ndcg_sum  = 0.0\n    mrr_sum   = 0.0\n    total     = 0\n\n    for uid in test_pool:\n        # Target = most recent item in test set for this user\n        user_test = df_test_valid[\n            df_test_valid['UserId'] == uid\n        ].sort_values('Time')\n\n        if user_test.empty:\n            continue\n\n        target = user_test.iloc[-1]['ProductId']\n        seen   = set(df_train[df_train['UserId'] == uid]['ProductId'])\n\n        try:\n            recs = rec_fn(uid, n=k, seen=seen)\n        except Exception:\n            total += 1\n            continue\n\n        total += 1\n\n        if target in recs:\n            hits     += 1\n            rank      = recs.index(target) + 1   # 1-indexed\n            ndcg_sum += 1.0 / np.log2(rank + 1)  # NDCG contribution\n            mrr_sum  += 1.0 / rank                # MRR contribution\n\n    hr   = hits / total     if total > 0 else 0.0\n    ndcg = ndcg_sum / total if total > 0 else 0.0\n    mrr  = mrr_sum / total  if total > 0 else 0.0\n\n    return {\n        'System'   : label,\n        f'HR@{k}'  : round(hr,   4),\n        f'NDCG@{k}': round(ndcg, 4),\n        'MRR'      : round(mrr,  4),\n        'Hits'     : hits,\n        'Total'    : total\n    }\n\n# ‚îÄ‚îÄ Run evaluation on all 4 systems ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"  Evaluating System 1: Popularity Baseline...\")\nres_pop    = evaluate_at_k(recommend_popular,  'Popularity Baseline')\nprint(f\"  ‚úÖ Done  HR@{K}={res_pop[f'HR@{K}']}\")\n\nprint(\"  Evaluating System 2: User-Based CF...\")\nres_cf     = evaluate_at_k(recommend_user_cf,  'User-Based CF')\nprint(f\"  ‚úÖ Done  HR@{K}={res_cf[f'HR@{K}']}\")\n\nprint(\"  Evaluating System 3: SVD...\")\nres_svd    = evaluate_at_k(recommend_svd,      'SVD (Matrix Factorization)')\nprint(f\"  ‚úÖ Done  HR@{K}={res_svd[f'HR@{K}']}\")\n\nprint(\"  Evaluating System 4: Hybrid...\")\nres_hybrid = evaluate_at_k(recommend_hybrid,   'Hybrid (SVD+CF+CB+Pop)')\nprint(f\"  ‚úÖ Done  HR@{K}={res_hybrid[f'HR@{K}']}\")\n\n# ‚îÄ‚îÄ Results table ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nresults = pd.DataFrame([res_pop, res_cf, res_svd, res_hybrid])\nresults = results.sort_values(f'HR@{K}', ascending=False).reset_index(drop=True)\n\n# Random baseline for context\nrandom_baseline = K / len(all_products)\n\nprint(f\"\\n{'='*72}\")\nprint(f\"  RANKING EVALUATION RESULTS  (K={K}, {N_EVAL} test users)\")\nprint(f\"{'='*72}\")\nprint(f\"  {'System':<32} {f'HR@{K}':>8} {f'NDCG@{K}':>9} {'MRR':>8} {'Hits':>7}\")\nprint(f\"  {'‚îÄ'*68}\")\nmedals = ['ü•á', 'ü•à', 'ü•â', '  ']\nfor i, row in results.iterrows():\n    print(f\"  {medals[i]} {row['System']:<30} \"\n          f\"{row[f'HR@{K}']:>8.4f} \"\n          f\"{row[f'NDCG@{K}']:>9.4f} \"\n          f\"{row['MRR']:>8.4f} \"\n          f\"{row['Hits']:>7.0f}\")\nprint(f\"{'='*72}\")\n\n# Random baseline comparison\nbest_hr = results.iloc[0][f'HR@{K}']\nprint(f\"\\n  Random baseline HR@{K}   : {random_baseline:.4f}\")\nprint(f\"  Best system HR@{K}       : {best_hr:.4f}\")\nprint(f\"  Improvement over random  : {best_hr/random_baseline:.1f}√ó\")\nprint(f\"\\n  Best system overall      : {results.iloc[0]['System']}\")\n\n# ‚îÄ‚îÄ Visualizations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\npalette   = ['#2ecc71', '#3498db', '#e67e22', '#e74c3c']\nsnames    = results['System'].values\n\nfor ax, col, title, note in zip(\n    axes,\n    [f'HR@{K}', f'NDCG@{K}', 'MRR'],\n    [f'Hit Rate @ {K}', f'NDCG @ {K}', 'Mean Reciprocal Rank'],\n    [\n        'Did we find the right item?',\n        'Did we rank it near the top?',\n        'How early in the list?'\n    ]\n):\n    bars = ax.bar(\n        snames,\n        results[col],\n        color=palette[:len(snames)],\n        edgecolor='white',\n        linewidth=1.5\n    )\n    ax.set_title(f'{title}\\n({note})',\n                 fontsize=11, fontweight='bold')\n    ax.set_ylabel(col)\n    ax.tick_params(axis='x', rotation=25)\n\n    # Value labels on bars\n    for bar, v in zip(bars, results[col]):\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.001,\n            f'{v:.4f}',\n            ha='center', va='bottom',\n            fontsize=9, fontweight='bold'\n        )\n\n    # Random baseline reference line\n    ax.axhline(\n        random_baseline,\n        color='red',\n        linewidth=1.5,\n        linestyle='--',\n        alpha=0.7,\n        label=f'Random ({random_baseline:.4f})'\n    )\n    ax.legend(fontsize=8)\n\nplt.suptitle(\n    f'Block 12 ‚Äî Recommendation System Ranking Evaluation\\n'\n    f'(K={K}, {N_EVAL} test users, Leave-One-Out protocol)',\n    fontsize=13, fontweight='bold', y=1.02\n)\nplt.tight_layout()\nplt.show()\n\n# ‚îÄ‚îÄ Detailed breakdown ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\nDetailed metric interpretation:\")\nbest = results.iloc[0]\nprint(f\"\\n  Best system: {best['System']}\")\nprint(f\"  HR@{K}  = {best[f'HR@{K}']:.4f}\")\nprint(f\"    ‚Üí In {best[f'HR@{K}']*100:.1f}% of cases we recommended\")\nprint(f\"      the correct product in our top-{K} list\")\nprint(f\"    ‚Üí {best[f'HR@{K}']/random_baseline:.1f}√ó better than random\")\nprint(f\"\\n  NDCG@{K} = {best[f'NDCG@{K}']:.4f}\")\nprint(f\"    ‚Üí When we find the correct item, it appears\")\nprint(f\"      near the top of the list on average\")\nprint(f\"\\n  MRR    = {best['MRR']:.4f}\")\nif best['MRR'] > 0:\n    avg_rank = 1 / best['MRR']\n    print(f\"    ‚Üí Correct item appears at rank ~{avg_rank:.1f} on average\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:39:48.513396Z","iopub.execute_input":"2026-02-27T18:39:48.514032Z","iopub.status.idle":"2026-02-27T18:41:21.033123Z","shell.execute_reply.started":"2026-02-27T18:39:48.514004Z","shell.execute_reply":"2026-02-27T18:41:21.032028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results ‚Äî Ranking Evaluation\n\n**Final ranking of all 4 systems:**\n\n| Rank | System | HR@10 | NDCG@10 | MRR | Hits/500 |\n|------|--------|-------|---------|-----|----------|\n| ü•á | User-Based CF | 0.0260 | 0.0141 | 0.0106 | 13 |\n| ü•à | Hybrid | 0.0180 | 0.0095 | 0.0069 | 9 |\n| ü•â | SVD | 0.0120 | 0.0060 | 0.0042 | 6 |\n| 4th | Popularity | 0.0020 | 0.0006 | 0.0002 | 1 |\n\n**Random baseline: 0.0020 ‚Üí Best system is 12.8√ó better ‚úÖ**\n\n---\n\n**Surprise finding: User CF beats the Hybrid**\n\nThe Hybrid assigns weight 0.40 to SVD ‚Äî our dominant signal.\nBut SVD (HR@10=0.012) is weaker than CF (HR@10=0.026)\non this specific dataset.\n\nBy over-weighting the weaker system, the Hybrid\ngets pulled away from the stronger CF signal.\n\n**Production fix:** tune weights via offline evaluation:\n```\nCurrent  : SVD=0.40, CF=0.25 ‚Üí Hybrid HR@10=0.018\nBetter   : SVD=0.25, CF=0.45 ‚Üí expected improvement\n```\nThis is exactly what the A/B test in Block 13 validates.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# **A/B TEST SIMULATION**","metadata":{}},{"cell_type":"markdown","source":"\n### The Production Question\n\nBuilding a better recommendation system is only half the job.\n**Proving it is better** is the other half.\n\nIn production at every FAANG company, no model gets deployed\nwithout passing an A/B test. The workflow is:\n```\nStep 1: Split users randomly into two groups\n        Group A (Control)   ‚Üí old system\n        Group B (Treatment) ‚Üí new system\n\nStep 2: Run both systems for 2 weeks\n        Measure: click rate, purchase rate, watch time\n\nStep 3: Statistical test\n        Is the difference real or just random noise?\n\nStep 4: Decision\n        p < 0.05 AND meaningful effect ‚Üí deploy\n        Otherwise                      ‚Üí do not deploy\n```\n\nWe simulate this exact workflow on our test data.\n\n---\n\n### What We Are Testing\n\nBlock 12 revealed a surprise:\n```\nü•á User-Based CF  : HR@10 = 0.0260  (best!)\nü•à Hybrid         : HR@10 = 0.0180\nü•â SVD            : HR@10 = 0.0120\n   Popularity     : HR@10 = 0.0020  (worst)\n```\n\nWe run TWO A/B tests:\n\n**Test 1: Popularity (A) vs User-CF (B)**\nThe classic test ‚Äî does personalization beat no personalization?\nExpected result: CF wins convincingly.\n\n**Test 2: SVD (A) vs User-CF (B)**\nThe interesting test ‚Äî does the neighborhood model beat\nthe matrix factorization model on our sparse dataset?\nExpected result: CF wins, but is it statistically significant?\n\n---\n\n### Statistical Test 1: Two-Sample T-Test\n```\nH‚ÇÄ (null hypothesis)      : mean(Group A) = mean(Group B)\nH‚ÇÅ (alternative)          : mean(Group B) > mean(Group A)\n\nClick signal per user:\n  HIT  = 1  (we recommended the correct product)\n  MISS = 0  (we did not recommend the correct product)\n\nTest statistic:\n  t = (mean_B - mean_A) / (pooled_std / ‚àön)\n\nDecision rule:\n  p < 0.05 ‚Üí reject H‚ÇÄ ‚Üí difference is real (95% confidence)\n  p ‚â• 0.05 ‚Üí fail to reject H‚ÇÄ ‚Üí could be random noise\n```\n\n**Assumption:** ratings are approximately normally distributed.\nWith binary data (0/1), this assumption is technically violated.\nThis is why we always run a second, more robust test.\n\n---\n\n### Statistical Test 2: Mann-Whitney U Test\n```\nDoes NOT assume normal distribution.\nWorks directly on binary data (0s and 1s).\nTests whether one group tends to have higher values.\n\nMore appropriate for our data than the t-test.\nAlways report BOTH:\n  T-test    ‚Üí familiar to most readers\n  Mann-Whitney ‚Üí correct for binary data\n```\n\nA result is truly robust only when BOTH tests agree.\nIf they disagree ‚Üí investigate why before deciding.\n\n---\n\n### Statistical Test 3: Cohen's d (Effect Size)\n\nThis is the most important and most overlooked metric.\n```\nd = (mean_B - mean_A) / pooled_standard_deviation\n\nInterpretation:\n  |d| < 0.20 : Negligible  ‚Äî statistically significant but meaningless\n  |d| = 0.20 : Small       ‚Äî noticeable with careful measurement\n  |d| = 0.50 : Medium      ‚Äî visible to a careful observer\n  |d| = 0.80 : Large       ‚Äî obvious to anyone\n  |d| > 1.00 : Very large  ‚Äî dramatic difference\n```\n\n**Why Cohen's d matters more than p-value:**\n\nWith 10 million users, even a 0.0001% improvement\nhas p < 0.001 (highly significant).\nBut is it worth the engineering cost to deploy?\n```\nScenario A: p=0.001, Cohen's d=0.02 ‚Üí statistically significant\n            but practically negligible ‚Üí do NOT deploy\n\nScenario B: p=0.04,  Cohen's d=0.35 ‚Üí borderline significance\n            but medium practical effect ‚Üí worth further testing\n```\n\n**FAANG rule:** report BOTH p-value AND effect size.\nA result with no effect size is incomplete analysis.\n\n---\n\n### The Decision Framework\n```\n                    Cohen's d < 0.2      Cohen's d ‚â• 0.2\n                    (small effect)       (meaningful effect)\n                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\np < 0.05        ‚îÇ Significant but      ‚îÇ ‚úÖ DEPLOY            ‚îÇ\n(significant)   ‚îÇ practically trivial  ‚îÇ Real AND meaningful  ‚îÇ\n                ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\np ‚â• 0.05        ‚îÇ ‚ùå DO NOT DEPLOY     ‚îÇ ‚ö†Ô∏è COLLECT MORE DATA ‚îÇ\n(not signif.)   ‚îÇ Random noise         ‚îÇ Effect exists but    ‚îÇ\n                ‚îÇ                      ‚îÇ underpowered test    ‚îÇ\n                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n","metadata":{}},{"cell_type":"code","source":"\n# A/B TEST SIMULATION\n\n\nprint(\"Running A/B Test Simulation...\")\nprint(\"Test 1: Popularity (A) vs User-CF (B)\")\nprint(\"Test 2: SVD (A)        vs User-CF (B)\")\nprint(\"=\" * 60)\n\n# ‚îÄ‚îÄ Helper: get click signals for a group of users ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef get_click_signals(uids, rec_fn, k=10, sample=250):\n    \"\"\"\n    For each user, simulate a \"click\":\n      HIT  = 1 if their actual next item is in top-K recs\n      MISS = 0 otherwise\n\n    Returns list of 0/1 values ‚Äî one per user.\n    \"\"\"\n    np.random.seed(42)\n    sample_uids = np.random.choice(\n        uids, min(sample, len(uids)), replace=False\n    )\n    clicks = []\n    for uid in sample_uids:\n        user_test = df_test_valid[\n            df_test_valid['UserId'] == uid\n        ].sort_values('Time')\n        if user_test.empty:\n            continue\n        target = user_test.iloc[-1]['ProductId']\n        seen   = set(df_train[df_train['UserId'] == uid]['ProductId'])\n        try:\n            recs = rec_fn(uid, n=k, seen=seen)\n            clicks.append(1 if target in recs else 0)\n        except Exception:\n            clicks.append(0)\n    return np.array(clicks)\n\n# ‚îÄ‚îÄ Helper: run one full A/B test ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef run_ab_test(name_a, rec_fn_a, name_b, rec_fn_b,\n                test_uids, sample=250, k=10):\n    \"\"\"\n    Split test_uids into two groups.\n    Measure hit rate for each group.\n    Run t-test, Mann-Whitney U, and Cohen's d.\n    Print full results and return metrics.\n    \"\"\"\n    np.random.seed(42)\n    shuffled = test_uids.copy()\n    np.random.shuffle(shuffled)\n    mid     = len(shuffled) // 2\n    group_a = shuffled[:mid]\n    group_b = shuffled[mid:]\n\n    print(f\"\\nCollecting signals for Group A ({name_a})...\")\n    clicks_a = get_click_signals(group_a, rec_fn_a, k=k, sample=sample)\n    print(f\"Collecting signals for Group B ({name_b})...\")\n    clicks_b = get_click_signals(group_b, rec_fn_b, k=k, sample=sample)\n\n    mean_a  = np.mean(clicks_a)\n    mean_b  = np.mean(clicks_b)\n    lift    = (mean_b - mean_a) / mean_a * 100 if mean_a > 0 else float('inf')\n\n    # ‚îÄ‚îÄ Statistical tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    # Test 1: Two-sample t-test (parametric)\n    t_stat, p_ttest = stats.ttest_ind(\n        clicks_b, clicks_a, alternative='greater'\n    )\n\n    # Test 2: Mann-Whitney U (non-parametric, better for binary)\n    u_stat, p_mw = stats.mannwhitneyu(\n        clicks_b, clicks_a, alternative='greater'\n    )\n\n    # Test 3: Cohen's d (effect size)\n    std_pool = np.sqrt(\n        (np.std(clicks_a)**2 + np.std(clicks_b)**2) / 2\n    )\n    cohens_d = (mean_b - mean_a) / (std_pool + 1e-10)\n\n    # ‚îÄ‚îÄ Effect size label ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    abs_d = abs(cohens_d)\n    if   abs_d < 0.20: effect_label = 'Negligible'\n    elif abs_d < 0.50: effect_label = 'Small'\n    elif abs_d < 0.80: effect_label = 'Medium'\n    elif abs_d < 1.00: effect_label = 'Large'\n    else:              effect_label = 'Very Large'\n\n    # ‚îÄ‚îÄ Decision ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    sig   = p_mw < 0.05\n    meaningful = abs_d >= 0.20\n    if sig and meaningful:\n        decision = f\"‚úÖ DEPLOY {name_b} ‚Äî significant AND meaningful\"\n    elif sig and not meaningful:\n        decision = f\"‚ö†Ô∏è  Significant but negligible effect ‚Äî reconsider\"\n    elif not sig and meaningful:\n        decision = f\"‚ö†Ô∏è  Meaningful effect but not significant ‚Äî need more data\"\n    else:\n        decision = f\"‚ùå DO NOT DEPLOY ‚Äî not significant, not meaningful\"\n\n    # ‚îÄ‚îÄ Print results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    print(f\"\\n{'='*60}\")\n    print(f\"  A/B TEST: {name_a} vs {name_b}\")\n    print(f\"{'='*60}\")\n    print(f\"  Group A ({name_a:<20}): \"\n          f\"hit rate = {mean_a:.4f}  (N={len(clicks_a)})\")\n    print(f\"  Group B ({name_b:<20}): \"\n          f\"hit rate = {mean_b:.4f}  (N={len(clicks_b)})\")\n    print(f\"\\n  Relative lift         : {lift:+.1f}%\")\n    print(f\"\\n  T-test   p-value      : {p_ttest:.4f}  \"\n          f\"{'‚úÖ Significant' if p_ttest < 0.05 else '‚ùå Not significant'}\")\n    print(f\"  Mann-Whitney p-value  : {p_mw:.4f}  \"\n          f\"{'‚úÖ Significant' if p_mw < 0.05 else '‚ùå Not significant'}\")\n    print(f\"  Cohen's d             : {cohens_d:.4f}  \"\n          f\"({effect_label})\")\n    print(f\"\\n  DECISION: {decision}\")\n    print(f\"{'='*60}\")\n\n    return {\n        'name_a'   : name_a,\n        'name_b'   : name_b,\n        'mean_a'   : mean_a,\n        'mean_b'   : mean_b,\n        'lift'     : lift,\n        'p_ttest'  : p_ttest,\n        'p_mw'     : p_mw,\n        'cohens_d' : cohens_d,\n        'effect'   : effect_label,\n        'decision' : decision,\n        'clicks_a' : clicks_a,\n        'clicks_b' : clicks_b\n    }\n\n# ‚îÄ‚îÄ Get test users ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nnp.random.seed(42)\ntest_uids = df_test_valid['UserId'].unique()\n\n# ‚îÄ‚îÄ Run Test 1: Popularity vs CF ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nab1 = run_ab_test(\n    'Popularity', recommend_popular,\n    'User-CF',    recommend_user_cf,\n    test_uids,    sample=250\n)\n\n# ‚îÄ‚îÄ Run Test 2: SVD vs CF ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nab2 = run_ab_test(\n    'SVD',      recommend_svd,\n    'User-CF',  recommend_user_cf,\n    test_uids,  sample=250\n)\n\n# ‚îÄ‚îÄ Visualizations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfor row, ab, title in zip(\n    [0, 1],\n    [ab1, ab2],\n    ['Test 1: Popularity vs User-CF',\n     'Test 2: SVD vs User-CF']\n):\n    name_a   = ab['name_a']\n    name_b   = ab['name_b']\n    clicks_a = ab['clicks_a']\n    clicks_b = ab['clicks_b']\n\n    # Chart 1: Hit rate comparison bar chart\n    axes[row, 0].bar(\n        [f'Group A\\n({name_a})', f'Group B\\n({name_b})'],\n        [ab['mean_a'], ab['mean_b']],\n        color=['#e74c3c', '#2ecc71'],\n        edgecolor='white', linewidth=2, width=0.5\n    )\n    axes[row, 0].set_ylabel('Hit Rate @ 10')\n    axes[row, 0].set_title(\n        f'{title}\\nHit Rate  |  lift={ab[\"lift\"]:+.1f}%',\n        fontweight='bold'\n    )\n    for i, v in enumerate([ab['mean_a'], ab['mean_b']]):\n        axes[row, 0].text(\n            i, v + 0.0005, f'{v:.4f}',\n            ha='center', fontweight='bold', fontsize=11\n        )\n\n    # Chart 2: p-value visualization\n    p_vals  = [ab['p_ttest'], ab['p_mw']]\n    p_names = ['T-test', 'Mann-Whitney']\n    colors  = ['#2ecc71' if p < 0.05 else '#e74c3c' for p in p_vals]\n    axes[row, 1].barh(p_names, p_vals, color=colors,\n                      edgecolor='white', linewidth=1.5)\n    axes[row, 1].axvline(\n        0.05, color='black', linewidth=2,\n        linestyle='--', label='Œ± = 0.05'\n    )\n    axes[row, 1].set_xlabel('p-value')\n    axes[row, 1].set_title(\n        f'Statistical Significance\\n'\n        f'(left of dashed line = significant)',\n        fontweight='bold'\n    )\n    axes[row, 1].legend()\n    for i, (p, name) in enumerate(zip(p_vals, p_names)):\n        axes[row, 1].text(\n            p + 0.001, i,\n            f'{p:.4f}', va='center',\n            fontsize=10, fontweight='bold'\n        )\n\n    # Chart 3: Cohen's d gauge\n    d_val    = ab['cohens_d']\n    d_levels = [0.0, 0.2, 0.5, 0.8, 1.0, max(1.2, abs(d_val) + 0.1)]\n    d_labels = ['Negligible', 'Small', 'Medium', 'Large', 'Very Large']\n    d_colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71', '#27ae60']\n    lefts    = [0.0]\n    for i in range(len(d_levels) - 1):\n        width = d_levels[i+1] - d_levels[i]\n        axes[row, 2].barh(\n            0, width, left=lefts[-1],\n            color=d_colors[i] if i < len(d_colors) else '#27ae60',\n            edgecolor='white', height=0.4, alpha=0.7\n        )\n        lefts.append(lefts[-1] + width)\n    axes[row, 2].axvline(\n        abs(d_val), color='black',\n        linewidth=3, linestyle='-',\n        label=f\"Cohen's d = {d_val:.4f}\\n({ab['effect']})\"\n    )\n    axes[row, 2].set_xlabel(\"Cohen's d (Effect Size)\")\n    axes[row, 2].set_title(\n        f\"Effect Size\\n({ab['effect']})\",\n        fontweight='bold'\n    )\n    axes[row, 2].set_yticks([])\n    axes[row, 2].legend(fontsize=9)\n    axes[row, 2].set_xlim(0, max(1.2, abs(d_val) + 0.1))\n\nplt.suptitle(\n    'Block 13 ‚Äî A/B Test Simulation\\n'\n    '(T-test + Mann-Whitney U + Cohen\\'s d)',\n    fontsize=14, fontweight='bold', y=1.02\n)\nplt.tight_layout()\nplt.show()\n\n# ‚îÄ‚îÄ Summary table ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n{'='*70}\")\nprint(f\"  A/B TEST SUMMARY\")\nprint(f\"{'='*70}\")\nprint(f\"  {'Test':<30} {'Lift':>8} {'p-MW':>8} {'Cohen d':>10} {'Decision':>6}\")\nprint(f\"  {'‚îÄ'*66}\")\nfor ab in [ab1, ab2]:\n    test_name = f\"{ab['name_a']} vs {ab['name_b']}\"\n    sig_icon  = '‚úÖ' if ab['p_mw'] < 0.05 else '‚ùå'\n    print(f\"  {test_name:<30} {ab['lift']:>+7.1f}% \"\n          f\"{ab['p_mw']:>8.4f} \"\n          f\"{ab['cohens_d']:>10.4f}  {sig_icon}\")\nprint(f\"{'='*70}\")\nprint(f\"\\n‚úÖ Block 13 complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:41:21.034521Z","iopub.execute_input":"2026-02-27T18:41:21.03485Z","iopub.status.idle":"2026-02-27T18:41:46.588646Z","shell.execute_reply.started":"2026-02-27T18:41:21.034824Z","shell.execute_reply":"2026-02-27T18:41:46.587735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Block 13 Results ‚Äî A/B Test Simulation\n\n**Summary:**\n| Test | Lift | p-value | Cohen's d | Decision |\n|------|------|---------|-----------|----------|\n| Popularity vs CF | +‚àû% | 0.0225 | 0.1803 | ‚ö†Ô∏è Significant, small effect |\n| SVD vs CF | +0.0% | 0.5006 | 0.0000 | ‚ùå No difference detected |\n\n---\n\n**Test 1: Popularity vs User-CF**\n\nBoth tests agree (p=0.022): CF is statistically better\nthan Popularity. However Cohen's d=0.18 indicates\nthe effect size is negligible in this sample.\n\nThe +‚àû% lift occurs because Popularity scored exactly\n0 hits in 250 users ‚Äî consistent with its HR@10=0.002\nfrom Block 12 (1 hit per 500 users on average).\n\n**Production decision:** Deploy CF over Popularity.\nEven a small absolute improvement matters at scale.\nWith 1 million users: 0.016 √ó 1M = 16,000 successful\nrecommendations vs 0 from pure popularity.\n\n---\n\n**Test 2: SVD vs User-CF**\n\nBoth systems scored identically (hit rate = 0.016).\nT-test p=0.50, Cohen's d=0.00 ‚Äî no difference detected.\n\nThis does NOT mean they are equal in reality.\nBlock 12 showed CF=0.026 vs SVD=0.012 on 500 users.\nThe contradiction is explained by sample size:\n```\nTo detect HR difference of 0.014 with 80% power:\n  Required: ~2,000+ users per group\n  We have : 250 users per group ‚Üí underpowered\n```\n\n**Production decision:** Do not deploy based on this test.\nCollect more data (2,000+ users) before deciding\nbetween SVD and CF for this specific dataset.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **POPULARITY BIAS AUDIT (Fairness Analysis)**","metadata":{}},{"cell_type":"markdown","source":"\n\n### The Problem Nobody Talks About in Tutorials\n\nEvery tutorial builds a recommendation system and stops\nat accuracy metrics. HR@10 looks good. \n\nBut accuracy alone hides a dangerous problem:\n**the system might be accurate AND deeply unfair.**\n\n---\n\n### What Is Popularity Bias?\n```\nMore ratings ‚Üí more signal ‚Üí model learns product better\n‚Üí recommends it more often ‚Üí gets even more ratings\n‚Üí even more signal ‚Üí recommended even more...\n```\n\nThis is a **self-reinforcing feedback loop.**\nProducts that are already popular get more exposure.\nProducts that are new or niche never get a chance.\n\n**In a food marketplace this means:**\n```\n‚ùå Small artisan coffee roaster   ‚Üí never recommended\n‚ùå New organic snack brand        ‚Üí never recommended  \n‚ùå Niche dietary product          ‚Üí never recommended\n\n‚úÖ Established brand with 500 reviews ‚Üí always recommended\n‚úÖ Viral product ‚Üí dominates all systems\n```\n\nThis is unfair to small vendors AND reduces\nrecommendation diversity for users.\nAmazon, Etsy, and Spotify all have dedicated\nfairness teams working on exactly this problem.\n\n---\n\n### Why This Matters for FAANG Interviews\n\nPost-2022, every major tech company has added\n\"responsible AI\" and \"fairness\" to their ML job requirements.\n\nBeing able to say:\n> *\"I measured popularity bias explicitly and found that\n> User-CF has the least bias despite being our best\n> performing system\"*\n\n...is exactly the kind of insight that gets you hired.\n\n---\n\n### How We Measure It\n\nWe sample recommendations from all 4 systems and measure\nwhat fraction go to products in each popularity tier:\n```\nTier          Definition              Products    Characteristic\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nHead          Top 5% by reviews       ~247        Blockbusters\nTorso         Top 5‚Äì20%               ~740        Moderately popular\nTail          Top 20‚Äì50%              ~1,479      Niche but known\nLong Tail     Bottom 50%              ~2,466      Obscure, underserved\n```\n\n**What a perfectly unbiased system would look like:**\n```\nHead     :  5% of recommendations  (proportional to catalog share)\nTorso    : 15% of recommendations\nTail     : 30% of recommendations\nLong Tail: 50% of recommendations  ‚Üê majority of catalog\n```\n\n**What a biased system looks like:**\n```\nHead     : 60%+ of recommendations  ‚Üê 12√ó over-represented\nLong Tail:  5% of recommendations   ‚Üê 10√ó under-represented\n```\n\n---\n\n### The Tension We Expect to Find\n\nBlock 12 told us:\n```\nü•á User-CF  : HR@10 = 0.0260  (most accurate)\nü•â SVD      : HR@10 = 0.0120  (less accurate)\n   Popularity: HR@10 = 0.0020  (least accurate)\n```\n\nThe question this block answers:\n> **Does the most accurate system also have the most bias?**\n> Or can we have accuracy AND fairness simultaneously?\n\nThis tension ‚Äî accuracy vs fairness ‚Äî is one of the\ncentral debates in modern ML ethics.\n\n---\n\n### The Four Systems Under the Microscope\n\n| System | Expected Bias | Reason |\n|--------|--------------|--------|\n| Popularity | üî¥ Highest | Literally ranks by popularity |\n| SVD | üü° Medium | Latent factors partially independent of popularity |\n| User CF | üü¢ Lowest? | Neighborhood signal follows user taste not popularity |\n| Hybrid | üü° Medium | Weighted combination ‚Äî inherits some bias from each |\n\nThese are predictions. The audit will confirm or surprise us.\n\n","metadata":{}},{"cell_type":"code","source":"\n# BLOCK 14 ‚Äî POPULARITY BIAS AUDIT (FAIRNESS ANALYSIS)\n\n\nprint(\"Running Popularity Bias Audit...\")\nprint(\"Auditing all 4 systems for recommendation fairness\\n\")\n\n# ‚îÄ‚îÄ Step 1: Categorize every product into popularity tiers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprod_review_counts = (df_train['ProductId']\n                      .value_counts()\n                      .reset_index())\nprod_review_counts.columns = ['ProductId', 'n_reviews']\nprod_review_counts['pct_rank'] = prod_review_counts['n_reviews'].rank(\n    pct=True, ascending=True\n)\n\ndef get_tier(pct):\n    if   pct >= 0.95: return 'Head (top 5%)'\n    elif pct >= 0.80: return 'Torso (5‚Äì20%)'\n    elif pct >= 0.50: return 'Tail (20‚Äì50%)'\n    else:             return 'Long Tail (<50%)'\n\nprod_review_counts['tier'] = prod_review_counts['pct_rank'].apply(get_tier)\ntier_map = dict(zip(\n    prod_review_counts['ProductId'],\n    prod_review_counts['tier']\n))\n\n# Tier statistics\ntiers_order = [\n    'Head (top 5%)',\n    'Torso (5‚Äì20%)',\n    'Tail (20‚Äì50%)',\n    'Long Tail (<50%)'\n]\nprint(\"Catalog tier distribution:\")\nprint(f\"  {'Tier':<20} {'Products':>10} {'% Catalog':>12} {'Avg Reviews':>14}\")\nprint(\"  \" + \"‚îÄ\" * 60)\nfor tier in tiers_order:\n    mask       = prod_review_counts['tier'] == tier\n    n_prods    = mask.sum()\n    pct_cat    = n_prods / len(prod_review_counts) * 100\n    avg_rev    = prod_review_counts[mask]['n_reviews'].mean()\n    print(f\"  {tier:<20} {n_prods:>10,} {pct_cat:>11.1f}% {avg_rev:>14.1f}\")\n\n# ‚îÄ‚îÄ Step 2: Sample users and collect recommendations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nnp.random.seed(42)\nn_bias_users = 200\nbias_users   = np.random.choice(\n    df_test_valid['UserId'].unique(),\n    min(n_bias_users, df_test_valid['UserId'].nunique()),\n    replace=False\n)\n\nsystems_to_audit = [\n    ('Popularity Baseline', recommend_popular),\n    ('User-Based CF',       recommend_user_cf),\n    ('SVD',                 recommend_svd),\n    ('Hybrid',              recommend_hybrid),\n]\n\nprint(f\"\\nCollecting recommendations from {n_bias_users} users...\")\ntier_results    = {name: {t: 0 for t in tiers_order}\n                   for name, _ in systems_to_audit}\ntotal_recs      = {name: 0 for name, _ in systems_to_audit}\n\nfor name, rec_fn in systems_to_audit:\n    print(f\"  Auditing: {name}...\")\n    for uid in bias_users:\n        seen = set(df_train[df_train['UserId'] == uid]['ProductId'])\n        try:\n            recs = rec_fn(uid, n=10, seen=seen)\n            for pid in recs:\n                tier = tier_map.get(pid, 'Long Tail (<50%)')\n                tier_results[name][tier] += 1\n                total_recs[name]         += 1\n        except Exception:\n            pass\n\n# ‚îÄ‚îÄ Step 3: Print tier distribution table ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n{'='*75}\")\nprint(f\"  POPULARITY BIAS RESULTS ‚Äî Tier Distribution (% of recommendations)\")\nprint(f\"{'='*75}\")\nprint(f\"  {'System':<25} {'Head%':>8} {'Torso%':>8} \"\n      f\"{'Tail%':>8} {'LongTail%':>11} {'Total':>7}\")\nprint(f\"  {'‚îÄ'*70}\")\n\ntier_pcts = {}\nfor name, _ in systems_to_audit:\n    total = total_recs[name] + 1e-9\n    pcts  = {t: tier_results[name][t] / total * 100 for t in tiers_order}\n    tier_pcts[name] = pcts\n    print(f\"  {name:<25} \"\n          f\"{pcts[tiers_order[0]]:>8.1f} \"\n          f\"{pcts[tiers_order[1]]:>8.1f} \"\n          f\"{pcts[tiers_order[2]]:>8.1f} \"\n          f\"{pcts[tiers_order[3]]:>11.1f} \"\n          f\"{int(total):>7}\")\n\nprint(f\"{'='*75}\")\n\n# Fair baseline\nprint(f\"\\n  Fair baseline (proportional to catalog):\")\nprint(f\"  {'Head':>8} {'Torso':>8} {'Tail':>8} {'LongTail':>10}\")\nprint(f\"  {'5.0%':>8} {'15.0%':>8} {'30.0%':>8} {'50.0%':>10}\")\n\n# ‚îÄ‚îÄ Step 4: Bias score (distance from fair baseline) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfair = {\n    'Head (top 5%)'   : 5.0,\n    'Torso (5‚Äì20%)'   : 15.0,\n    'Tail (20‚Äì50%)'   : 30.0,\n    'Long Tail (<50%)': 50.0\n}\nprint(f\"\\n  Bias Score (distance from perfectly fair distribution):\")\nprint(f\"  Lower = more fair\\n\")\nprint(f\"  {'System':<25} {'Bias Score':>12} {'Verdict':>20}\")\nprint(f\"  {'‚îÄ'*60}\")\n\nbias_scores = {}\nfor name, _ in systems_to_audit:\n    score = sum(\n        abs(tier_pcts[name][t] - fair[t])\n        for t in tiers_order\n    )\n    bias_scores[name] = score\n    if   score < 30:  verdict = 'üü¢ Low bias'\n    elif score < 60:  verdict = 'üü° Medium bias'\n    else:             verdict = 'üî¥ High bias'\n    print(f\"  {name:<25} {score:>12.1f} {verdict:>20}\")\n\nleast_biased = min(bias_scores, key=bias_scores.get)\nmost_biased  = max(bias_scores, key=bias_scores.get)\nprint(f\"\\n  Most fair system  : {least_biased}\")\nprint(f\"  Most biased system: {most_biased}\")\n\n# ‚îÄ‚îÄ Step 5: Visualizations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(1, 3, figsize=(20, 7))\ntier_colors = ['#e74c3c', '#e67e22', '#3498db', '#2ecc71']\nx           = np.arange(len(systems_to_audit))\nsnames      = [n for n, _ in systems_to_audit]\n\n# Chart 1: Stacked bar ‚Äî tier distribution\nbottoms = np.zeros(len(systems_to_audit))\nfor tier, color in zip(tiers_order, tier_colors):\n    vals = [tier_pcts[n][tier] for n in snames]\n    axes[0].bar(x, vals, bottom=bottoms,\n                label=tier, color=color,\n                edgecolor='white', linewidth=0.8)\n    bottoms += np.array(vals)\n\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(snames, rotation=20, ha='right')\naxes[0].set_ylabel('% of Recommendations')\naxes[0].set_ylim(0, 110)\naxes[0].set_title('Tier Distribution per System\\n'\n                  '(More green = less biased)',\n                  fontweight='bold')\naxes[0].legend(loc='upper right', fontsize=8)\n\n# Add fair baseline line\naxes[0].axhline(50, color='black', linewidth=1.5,\n                linestyle='--', alpha=0.5,\n                label='50% = fair Long Tail')\n\n# Chart 2: Long Tail % only (the key fairness metric)\nlt_vals = [tier_pcts[n]['Long Tail (<50%)'] for n in snames]\nlt_colors = ['#2ecc71' if v >= 40\n             else '#f1c40f' if v >= 20\n             else '#e74c3c'\n             for v in lt_vals]\nbars2 = axes[1].bar(snames, lt_vals,\n                    color=lt_colors,\n                    edgecolor='white', linewidth=1.5)\naxes[1].axhline(50.0, color='black', linewidth=2,\n                linestyle='--', label='Fair baseline (50%)')\naxes[1].set_ylabel('% Long Tail Recommendations')\naxes[1].set_title('Long Tail Exposure\\n'\n                  '(Higher = more fair to niche products)',\n                  fontweight='bold')\naxes[1].tick_params(axis='x', rotation=20)\naxes[1].legend()\nfor bar, v in zip(bars2, lt_vals):\n    axes[1].text(\n        bar.get_x() + bar.get_width()/2,\n        bar.get_height() + 0.5,\n        f'{v:.1f}%', ha='center',\n        fontsize=10, fontweight='bold'\n    )\n\n# Chart 3: Bias score bar chart\nbs_vals   = [bias_scores[n] for n in snames]\nbs_colors = ['#2ecc71' if v < 30\n             else '#f1c40f' if v < 60\n             else '#e74c3c'\n             for v in bs_vals]\nbars3 = axes[2].bar(snames, bs_vals,\n                    color=bs_colors,\n                    edgecolor='white', linewidth=1.5)\naxes[2].set_ylabel('Bias Score (lower = more fair)')\naxes[2].set_title('Overall Bias Score\\n'\n                  '(Distance from perfectly fair distribution)',\n                  fontweight='bold')\naxes[2].tick_params(axis='x', rotation=20)\nfor bar, v in zip(bars3, bs_vals):\n    axes[2].text(\n        bar.get_x() + bar.get_width()/2,\n        bar.get_height() + 0.3,\n        f'{v:.1f}', ha='center',\n        fontsize=10, fontweight='bold'\n    )\n\nplt.suptitle(\n    'Block 14 ‚Äî Popularity Bias Audit: Fairness Analysis\\n'\n    'Does accuracy come at the cost of fairness?',\n    fontsize=13, fontweight='bold', y=1.02\n)\nplt.tight_layout()\nplt.show()\n\n# ‚îÄ‚îÄ Step 6: The accuracy vs fairness question ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n{'='*60}\")\nprint(f\"  ACCURACY vs FAIRNESS SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"  {'System':<25} {'HR@10':>8} {'Bias Score':>12} {'Verdict'}\")\nprint(f\"  {'‚îÄ'*58}\")\n\nhr_scores = {\n    'Popularity Baseline'   : 0.0020,\n    'User-Based CF'         : 0.0260,\n    'SVD'                   : 0.0120,\n    'Hybrid (SVD+CF+CB+Pop)': 0.0180,\n}\n# match keys\nfor name, _ in systems_to_audit:\n    hr  = hr_scores.get(name, 0)\n    bs  = bias_scores[name]\n    if   bs < 30:  fair_label = 'üü¢ Fair'\n    elif bs < 60:  fair_label = 'üü° Medium'\n    else:          fair_label = 'üî¥ Biased'\n    print(f\"  {name:<25} {hr:>8.4f} {bs:>12.1f}  {fair_label}\")\n\nprint(f\"{'='*60}\")\nprint(f\"\\n‚úÖ Block 14 complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:41:46.5905Z","iopub.execute_input":"2026-02-27T18:41:46.590897Z","iopub.status.idle":"2026-02-27T18:42:21.048207Z","shell.execute_reply.started":"2026-02-27T18:41:46.590868Z","shell.execute_reply":"2026-02-27T18:42:21.047012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results ‚Äî Popularity Bias Audit\n\n**Catalog structure:**\n| Tier | Products | Avg Reviews | Signal Strength |\n|------|----------|-------------|-----------------|\n| Head (5%) | 244 | 207.6 | 38√ó more than Long Tail |\n| Torso (5-20%) | 745 | 72.8 | 13√ó more |\n| Tail (20-50%) | 1,463 | 15.5 | 3√ó more |\n| Long Tail (<50%) | 2,480 | 5.4 | baseline |\n\n**Tier distribution of recommendations:**\n| System | Head% | Torso% | LongTail% | Bias Score |\n|--------|-------|--------|-----------|------------|\n| Popularity | 0.0 | 80.1 | 0.0 | 130.2 üî¥ |\n| User CF | 26.2 | 48.9 | 8.3 | 110.3 üî¥ |\n| SVD | 37.3 | 33.1 | 23.2 | 100.8 üî¥ |\n| Hybrid | 56.3 | 36.6 | 0.8 | 145.9 üî¥ |\n\n**Three surprising findings:**\n\n**1. Popularity Baseline avoids Head products (0.0%)**\nThe Bayesian Average formula accidentally reduces Head bias\nby pulling high-raw-score products with few reviews toward\nthe global mean. Torso products (72 avg reviews) hit the\nsweet spot of trust + quality.\n\n**2. CF has the most Long Tail exposure (8.3%)**\nCF surfaces niche products because it follows user taste\nclusters ‚Äî some users have niche preferences that\nmap to Long Tail products. This is why CF is both\nour most accurate AND most fair system.\n**Accuracy and fairness CAN coexist.**\n\n**3. Hybrid is the most biased (145.9)**\nCombining SVD (Head-heavy) with Content-Based\n(favors text-rich = high-review products) amplifies\nHead bias instead of averaging it.\nThe Hybrid's popularity component (Torso-dominant)\nwas actually the fairness moderator.\n\n**Why all systems score High Bias:**\nWith matrix density of 0.195%, Head products have\n38√ó more signal than Long Tail products.\nBias is partly inherent to sparse collaborative filtering,\nnot just a modeling choice.\n\n**Production fix:**\nRe-ranking with diversity constraints:\nensure at least 20% of final recommendations\ncome from Tail or Long Tail products.\nThis is called \"fairness-aware re-ranking\"\nand is deployed at Spotify and Amazon today.","metadata":{}},{"cell_type":"markdown","source":"# **FINAL EXECUTIVE DASHBOARD**","metadata":{}},{"cell_type":"markdown","source":"\n### The Complete Picture in One View\n\nOver the previous 14 blocks we built, evaluated, and audited\nfour recommendation systems. This final dashboard brings\nevery finding together into one executive summary.\n```\nBlock 11 ‚Üí RMSE: how accurate are our rating predictions?\nBlock 12 ‚Üí HR@K, NDCG, MRR: do we recommend the right products?\nBlock 13 ‚Üí A/B test: are differences statistically real?\nBlock 14 ‚Üí Bias audit: are we fair to niche products?\n```\n\nThis is the chart you show in:\n- A **Kaggle presentation** to earn upvotes and medals\n- A **FAANG technical interview** when asked to summarize your work\n- A **GitHub README** as the preview image\n- A **blog post or publication** as the key figure\n\n---\n\n### What the Dashboard Reveals\n\nSix panels tell the complete story:\n\n| Panel | What It Shows | Key Finding |\n|-------|--------------|-------------|\n| HR@10 | Ranking accuracy | CF best (0.026) |\n| NDCG@10 | Position quality | CF best (0.014) |\n| A/B Test | Statistical proof | CF vs Pop significant |\n| Bias Score | Fairness | SVD most fair |\n| Long Tail % | Niche exposure | SVD best (23.2%) |\n| Capability Matrix | System properties | Use case guide |\n\n---\n\n### The Three Headline Findings\n\n**Finding 1: User-CF wins on accuracy**\nHR@10 = 0.026 ‚Äî 12.8√ó better than random.\nThe neighborhood model outperforms the more\nsophisticated SVD on our sparse dataset.\n\n**Finding 2: SVD wins on fairness**\nLowest bias score (100.8) with highest Long Tail\nexposure (23.2%) ‚Äî latent factors capture\nniche taste dimensions that benefit small vendors.\n\n**Finding 3: The Hybrid needs retuning**\nSVD weight (0.40) is too high for this dataset.\nIncreasing CF weight to 0.45 would likely improve\nboth accuracy AND fairness simultaneously.\n\n","metadata":{}},{"cell_type":"code","source":"\n# FINAL EXECUTIVE DASHBOARD\n\n\nprint(\"Building Final Executive Dashboard...\")\nprint(\"Combining all findings from Blocks 11‚Äì14\\n\")\n\n# ‚îÄ‚îÄ DEBUG: verify bias_scores keys before using them ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"Keys in bias_scores:\")\nfor k, v in bias_scores.items():\n    print(f\"  '{k}' : {v:.1f}\")\n\n# ‚îÄ‚îÄ Data from previous blocks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# Use EXACT keys from bias_scores and tier_pcts\npop_key    = 'Popularity Baseline'\ncf_key     = 'User-Based CF'\nsvd_key    = 'SVD'\nhybrid_key = 'Hybrid'   # ‚Üê fixed: matches what Block 14 stored\n\nsystems      = ['Popularity', 'User CF', 'SVD', 'Hybrid']\nhr_vals      = [\n    res_pop[f'HR@{K}'],\n    res_cf[f'HR@{K}'],\n    res_svd[f'HR@{K}'],\n    res_hybrid[f'HR@{K}']\n]\nndcg_vals    = [\n    res_pop[f'NDCG@{K}'],\n    res_cf[f'NDCG@{K}'],\n    res_svd[f'NDCG@{K}'],\n    res_hybrid[f'NDCG@{K}']\n]\nmrr_vals     = [\n    res_pop['MRR'],\n    res_cf['MRR'],\n    res_svd['MRR'],\n    res_hybrid['MRR']\n]\nbias_vals = [\n    bias_scores.get(pop_key,    0),\n    bias_scores.get(cf_key,     0),\n    bias_scores.get(svd_key,    0),\n    bias_scores.get(hybrid_key, 0),   # ‚Üê now uses correct key 'Hybrid'\n]\nlt_vals_dash = [\n    tier_pcts.get(pop_key,    {}).get('Long Tail (<50%)', 0),\n    tier_pcts.get(cf_key,     {}).get('Long Tail (<50%)', 0),\n    tier_pcts.get(svd_key,    {}).get('Long Tail (<50%)', 0),\n    tier_pcts.get(hybrid_key, {}).get('Long Tail (<50%)', 0),\n]\n\n# ‚îÄ‚îÄ Verify all values are correct before plotting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(f\"\\n‚úÖ Verified values for dashboard:\")\nprint(f\"  {'System':<12} {'HR@10':>8} {'Bias':>8} {'LongTail%':>11}\")\nprint(f\"  {'‚îÄ'*42}\")\nfor s, hr, bs, lt in zip(systems, hr_vals, bias_vals, lt_vals_dash):\n    print(f\"  {s:<12} {hr:>8.4f} {bs:>8.1f} {lt:>10.1f}%\")\n\n# Assert no bias value is 0 (would indicate key mismatch)\nassert all(v > 0 for v in bias_vals), \\\n    f\"‚ùå Bias values contain 0 ‚Äî key mismatch! Check keys above.\\n{bias_vals}\"\nprint(f\"\\n‚úÖ All bias values verified ‚Äî no zeros detected\\n\")\n\npal = ['#e74c3c', '#2ecc71', '#e67e22', '#3498db']\n\n# ‚îÄ‚îÄ Build dashboard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig = plt.figure(figsize=(22, 16))\ngs  = gridspec.GridSpec(3, 3, hspace=0.55, wspace=0.38)\n\n# ‚îÄ‚îÄ Panel 1: HR@K ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax1  = fig.add_subplot(gs[0, 0])\nbars = ax1.bar(systems, hr_vals, color=pal,\n               edgecolor='white', linewidth=1.5)\nax1.axhline(K / len(all_products), color='red',\n            linewidth=1.5, linestyle='--',\n            alpha=0.7, label='Random baseline')\nax1.set_title(f'Hit Rate @ {K}\\n(Higher = Better)',\n              fontweight='bold', fontsize=11)\nax1.set_ylabel(f'HR@{K}')\nax1.tick_params(axis='x', rotation=20)\nax1.legend(fontsize=8)\nfor bar, v in zip(bars, hr_vals):\n    ax1.text(bar.get_x() + bar.get_width()/2,\n             bar.get_height() + 0.0005,\n             f'{v:.4f}', ha='center',\n             fontsize=9, fontweight='bold')\nbest_hr_idx = hr_vals.index(max(hr_vals))\nax1.patches[best_hr_idx].set_edgecolor('gold')\nax1.patches[best_hr_idx].set_linewidth(3)\n\n# ‚îÄ‚îÄ Panel 2: NDCG@K ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax2  = fig.add_subplot(gs[0, 1])\nbars = ax2.bar(systems, ndcg_vals, color=pal,\n               edgecolor='white', linewidth=1.5)\nax2.set_title(f'NDCG @ {K}\\n(Rewards early ranking)',\n              fontweight='bold', fontsize=11)\nax2.set_ylabel(f'NDCG@{K}')\nax2.tick_params(axis='x', rotation=20)\nfor bar, v in zip(bars, ndcg_vals):\n    ax2.text(bar.get_x() + bar.get_width()/2,\n             bar.get_height() + 0.0002,\n             f'{v:.4f}', ha='center',\n             fontsize=9, fontweight='bold')\nax2.patches[ndcg_vals.index(max(ndcg_vals))].set_edgecolor('gold')\nax2.patches[ndcg_vals.index(max(ndcg_vals))].set_linewidth(3)\n\n# ‚îÄ‚îÄ Panel 3: MRR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax3  = fig.add_subplot(gs[0, 2])\nbars = ax3.bar(systems, mrr_vals, color=pal,\n               edgecolor='white', linewidth=1.5)\nax3.set_title('Mean Reciprocal Rank\\n(Higher = Better)',\n              fontweight='bold', fontsize=11)\nax3.set_ylabel('MRR')\nax3.tick_params(axis='x', rotation=20)\nfor bar, v in zip(bars, mrr_vals):\n    ax3.text(bar.get_x() + bar.get_width()/2,\n             bar.get_height() + 0.0002,\n             f'{v:.4f}', ha='center',\n             fontsize=9, fontweight='bold')\nax3.patches[mrr_vals.index(max(mrr_vals))].set_edgecolor('gold')\nax3.patches[mrr_vals.index(max(mrr_vals))].set_linewidth(3)\n\n# ‚îÄ‚îÄ Panel 4: A/B Test ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax4 = fig.add_subplot(gs[1, 0])\nab_names = [\n    f\"Pop\\n(HR={ab1['mean_a']:.4f})\",\n    f\"CF\\n(HR={ab1['mean_b']:.4f})\"\n]\nab_vals_ab = [ab1['mean_a'], ab1['mean_b']]\nab_cols    = ['#e74c3c', '#2ecc71']\nbars = ax4.bar(ab_names, ab_vals_ab, color=ab_cols,\n               edgecolor='white', linewidth=2, width=0.5)\nax4.set_title(\n    f'A/B Test: Pop vs CF\\n'\n    f'p={ab1[\"p_mw\"]:.4f} | d={ab1[\"cohens_d\"]:.3f}',\n    fontweight='bold', fontsize=11\n)\nax4.set_ylabel('Hit Rate @ 10')\nfor bar, v in zip(bars, ab_vals_ab):\n    ax4.text(bar.get_x() + bar.get_width()/2,\n             bar.get_height() + 0.0003,\n             f'{v:.4f}', ha='center',\n             fontsize=10, fontweight='bold')\nsig_text = '‚úÖ Significant' if ab1['p_mw'] < 0.05 else '‚ùå Not significant'\nax4.text(0.5, 0.95, sig_text,\n         transform=ax4.transAxes,\n         ha='center', va='top',\n         fontsize=9, fontweight='bold',\n         color='green' if ab1['p_mw'] < 0.05 else 'red')\n\n# ‚îÄ‚îÄ Panel 5: Bias Score ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax5      = fig.add_subplot(gs[1, 1])\nbs_colors = ['#e74c3c' for _ in bias_vals]   # all red ‚Äî all high bias\nbars = ax5.bar(systems, bias_vals, color=bs_colors,\n               edgecolor='white', linewidth=1.5)\nax5.set_title('Bias Score\\n(Lower = More Fair)',\n              fontweight='bold', fontsize=11)\nax5.set_ylabel('Bias Score')\nax5.tick_params(axis='x', rotation=20)\nfor bar, v in zip(bars, bias_vals):\n    ax5.text(bar.get_x() + bar.get_width()/2,\n             bar.get_height() + 0.5,\n             f'{v:.1f}', ha='center',\n             fontsize=9, fontweight='bold')\nbest_bias_idx = bias_vals.index(min(bias_vals))\nax5.patches[best_bias_idx].set_edgecolor('gold')\nax5.patches[best_bias_idx].set_linewidth(3)\n\n# ‚îÄ‚îÄ Panel 6: Long Tail Exposure ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax6       = fig.add_subplot(gs[1, 2])\nlt_colors = ['#2ecc71' if v >= 40\n             else '#f1c40f' if v >= 15\n             else '#e74c3c'\n             for v in lt_vals_dash]\nbars = ax6.bar(systems, lt_vals_dash, color=lt_colors,\n               edgecolor='white', linewidth=1.5)\nax6.axhline(50.0, color='black', linewidth=1.5,\n            linestyle='--', alpha=0.6,\n            label='Fair baseline (50%)')\nax6.set_title('Long Tail Exposure %\\n(Higher = More Fair)',\n              fontweight='bold', fontsize=11)\nax6.set_ylabel('% Long Tail Recommendations')\nax6.tick_params(axis='x', rotation=20)\nax6.legend(fontsize=8)\nfor bar, v in zip(bars, lt_vals_dash):\n    ax6.text(bar.get_x() + bar.get_width()/2,\n             bar.get_height() + 0.3,\n             f'{v:.1f}%', ha='center',\n             fontsize=9, fontweight='bold')\nbest_lt_idx = lt_vals_dash.index(max(lt_vals_dash))\nax6.patches[best_lt_idx].set_edgecolor('gold')\nax6.patches[best_lt_idx].set_linewidth(3)\n\n# ‚îÄ‚îÄ Panel 7: Capability Matrix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nax7 = fig.add_subplot(gs[2, :])\nax7.axis('off')\n\ntable_data = [\n    ['Criterion',        'Popularity',         'User CF',             'SVD',                    'Hybrid'],\n    ['Cold Start users', '‚úÖ Always works',     '‚ùå Needs history',    '‚ùå Needs history',        '‚úÖ Fallback to Pop'],\n    ['Cold Start prods', '‚ùå Ignores new',      '‚ùå Ignores new',      '‚ùå Ignores new',          '‚ö†Ô∏è CB covers text'],\n    ['Personalized?',    '‚ùå Same for all',     '‚úÖ Fully personal.',  '‚úÖ Fully personal.',      '‚úÖ Fully personal.'],\n    ['Training time',    '‚ö° Instant',          '‚ö° Instant',          'üê¢ Minutes',             'üê¢ Minutes'],\n    ['Query time',       '‚ö° Instant',          'üê¢ Slow (sim.)',      '‚ö° Row lookup',           'üê¢ Combines all'],\n    ['HR@10',            '0.0020',              '0.0260 ü•á',          '0.0120',                  '0.0180'],\n    ['Bias Score‚Üì',      f'{bias_vals[0]:.1f}', f'{bias_vals[1]:.1f}',f'{bias_vals[2]:.1f} ü•á', f'{bias_vals[3]:.1f}'],\n    ['Long Tail %',      f'{lt_vals_dash[0]:.1f}%', f'{lt_vals_dash[1]:.1f}%', f'{lt_vals_dash[2]:.1f}% ü•á', f'{lt_vals_dash[3]:.1f}%'],\n    ['Best use case',    'Cold start fallback', 'Active users',        'Sparse data+fairness',   'Dense data, tuned'],\n]\n\ntbl = ax7.table(\n    cellText  = table_data[1:],\n    colLabels = table_data[0],\n    cellLoc   = 'center',\n    loc       = 'center',\n    bbox      = [0, 0, 1, 1]\n)\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(9.5)\n\nfor (r, c), cell in tbl.get_celld().items():\n    cell.set_edgecolor('#bdc3c7')\n    if r == 0:\n        cell.set_facecolor('#2c3e50')\n        cell.set_text_props(color='white', fontweight='bold')\n    elif c == 0:\n        cell.set_facecolor('#ecf0f1')\n        cell.set_text_props(fontweight='bold')\n    elif r % 2 == 0:\n        cell.set_facecolor('#f8f9fa')\n    else:\n        cell.set_facecolor('white')\n\nax7.set_title('System Capability Matrix ‚Äî Complete Comparison',\n              fontweight='bold', fontsize=12, pad=12)\n\nplt.suptitle(\n    'Complete Recommendation System Dashboard ‚Äî 2026\\n'\n    'scipy.sparse SVD | NumPy 2.x | Amazon Fine Food Reviews',\n    fontsize=15, fontweight='bold', y=1.01\n)\n\nplt.savefig('recsys_dashboard_2026.png',\n            bbox_inches='tight', dpi=150)\nplt.show()\n\nprint(f\"\\n‚úÖ Dashboard saved: recsys_dashboard_2026.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T18:42:21.049839Z","iopub.execute_input":"2026-02-27T18:42:21.050206Z","iopub.status.idle":"2026-02-27T18:42:23.897292Z","shell.execute_reply.started":"2026-02-27T18:42:21.05018Z","shell.execute_reply":"2026-02-27T18:42:23.896204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **Conclusion:** ‚Äî What We Learned\n\n\n## Final Results\n\n| System | HR@10 | NDCG@10 | MRR | Bias Score‚Üì | Long Tail% |\n|--------|-------|---------|-----|-------------|-----------|\n| Popularity Baseline | 0.0020 | 0.0006 | 0.0002 | 130.2 üî¥ | 0.0% |\n| SVD (Matrix Factorization) | 0.0120 | 0.0060 | 0.0042 | 100.8 üî¥ | 23.2% |\n| Hybrid Ensemble | 0.0180 | 0.0095 | 0.0069 | 145.9 üî¥ | 0.8% |\n| **User-Based CF** | **0.0260** ü•á | **0.0141** ü•á | **0.0106** ü•á | 110.3 üî¥ | 8.3% |\n\nRandom baseline HR@10 = 0.0020. **Best system is 12.8√ó better than random.**\n\n---\n\n## The Five Findings Worth Remembering\n\n---\n\n### Finding 1 ‚Äî The Simpler Algorithm Won\n\nUser-Based Collaborative Filtering beats SVD Matrix Factorization on every ranking metric.\n\nThis is counterintuitive. SVD is more sophisticated ‚Äî it won the Netflix Prize and is deployed\nat Netflix, YouTube, and Amazon at scale. On *this* dataset, however:\n```\nMatrix density : 0.195%   (Netflix was ~5% ‚Äî 25√ó denser)\nSingular value ratio : 2.1√ó  (healthy = 15-20√ó)\nk=50 factors vs Netflix winners' k=200+\n```\n\nWhen matrix density is this low, SVD's latent factors are trained on thin signal.\nThe neighborhood model's direct approach ‚Äî \"users who rated what you rated also liked X\" ‚Äî\nis more reliable when overlap between users is sparse.\n\n> **The lesson:** Algorithm sophistication is not a substitute for data density.\n> Choose the algorithm that fits your data, not the one that won the most famous competition.\n\n---\n\n### Finding 2 ‚Äî The Hybrid Is Not Always Better\n```\nUser-Based CF  :  HR@10 = 0.0260  ü•á\nHybrid         :  HR@10 = 0.0180  ü•à  ‚Üê second despite combining all systems\n```\n\nThe Hybrid gives SVD its highest weight (0.40). But SVD is our *weakest* personalized system\non this dataset. By over-weighting the weakest component, the Hybrid is pulled away from CF ‚Äî\nits strongest signal.\n\nAdditionally:\n- SVD (weight 0.40) ‚Üí biased toward Head products\n- Content-Based (weight 0.20) ‚Üí biased toward text-rich = high-review = Head products\n- Combined weight on Head-biased signals: **0.65**\n\nThe Popularity component (Torso-dominant due to Bayesian Average) had only 0.15 weight\nand could not counterbalance them.\n\n**The fix in production:**\n```\nCurrent  : SVD=0.40, CF=0.25  ‚Üí  HR@10 = 0.018\nBetter   : SVD=0.25, CF=0.45  ‚Üí  expected improvement\n```\n\n> **The lesson:** A Hybrid is only as good as its weight tuning.\n> Always optimize weights via offline evaluation or A/B testing ‚Äî never fix them manually.\n\n---\n\n### Finding 3 ‚Äî Combining Systems Amplified Bias\n\nThe Hybrid's Head product exposure (56.3%) is **higher than any individual system**:\n```\nPopularity Baseline :  0.0% Head   (Torso-dominant due to Bayesian Average)\nUser-Based CF       : 26.2% Head\nSVD                 : 37.3% Head\nHybrid              : 56.3% Head   ‚Üê more than any component!\n```\n\nCombining systems did not average their biases. It amplified the dominant one.\nThree signals independently favor popular Head products.\nOne signal (Popularity, weight 0.15) cannot counterbalance them.\n\n**The production fix ‚Äî Fairness-aware re-ranking:**\nAfter generating the top-30 hybrid candidates, enforce a minimum 20% from Tail/Long Tail\nbefore presenting the final top-10.\nThis approach is deployed today at Spotify (Discover Weekly) and Amazon (fresh product promotion).\n\n---\n\n### Finding 4 ‚Äî Accuracy and Fairness Can Coexist\n\nThis was the question we set out to answer: *does the most accurate system also have the most bias?*\n```\nUser-Based CF :  HR@10 = 0.0260  (most accurate)\n               Bias  = 110.3    (second most fair)\n               Long Tail = 8.3% (highest among personalized systems)\n```\n\nCF's neighborhood signal follows genuine user taste. Some users genuinely love niche products.\nThis creates natural Long Tail exposure without any fairness intervention.\n\nSVD wins on fairness (Bias = 100.8, Long Tail = 23.2%) because its latent factors capture\nhidden taste dimensions ‚Äî dimensions that correspond to niche preferences like specialty diets,\nartisan brands, or regional foods.\n\n> **The lesson:** You do not have to choose between accuracy and fairness.\n> The right algorithm, honestly evaluated, can serve users and small vendors simultaneously.\n\n---\n\n### Finding 5 ‚Äî P-Value Without Effect Size Is Incomplete\n\n**A/B Test 1: Popularity vs User-CF**\n```\np = 0.0225  ‚úÖ Statistically significant (95% confidence)\nCohen's d = 0.18  ‚ö†Ô∏è Negligible effect size\n```\n\nBoth results are true simultaneously. The difference is real but small in absolute terms\n(0 hits vs 4 hits in 250 users). At scale ‚Äî one million users ‚Äî this translates to\n**16,000 additional successful recommendations per day.** Deploying CF is justified.\n\n**A/B Test 2: SVD vs User-CF**\n```\np = 0.5006  ‚ùå Not significant\nCohen's d = 0.00  (identical hit rates in this sample)\n```\n\nThis is *not* evidence that SVD and CF are equal. It is evidence of an underpowered test.\nTo detect the HR difference of 0.014 observed in Block 12 with 80% statistical power:\n```\nRequired sample size : ~2,000 users per group\nOur sample size      :   250 users per group  ‚Üí  underpowered by 8√ó\n```\n\n> **The lesson:** \"No effect found\" and \"effect not yet detectable\" are completely different conclusions.\n> Always report sample size, statistical power, AND effect size ‚Äî not just p-value.\n> This distinction is what FAANG interviewers test for.\n\n---\n\n## Honest Limitations\n\nA senior data scientist acknowledges what did not work:\n\n- **RMSE = 1.312** ‚Äî higher than published benchmarks. Caused by matrix density (0.195% vs ~5%)\n  and conservative factor count (k=50 vs winners' k=200+). The 4‚≠ê bucket achieves 0.778,\n  beating the Netflix Prize threshold (0.857) ‚Äî but overall accuracy is limited by sparsity.\n\n- **All systems show High Bias** ‚Äî every system has Bias Score > 100.\n  Popularity bias is partly *inherent* to collaborative filtering on sparse data:\n  products with more reviews have more signal. Fairness-aware re-ranking is the production fix.\n\n- **A/B test underpowered** ‚Äî 250 users per group is insufficient to reliably compare\n  systems separated by small HR differences. A production A/B test runs 2‚Äì4 weeks with thousands of users.\n\n- **Cold start unsolved** ‚Äî new users receive popularity-based recommendations.\n  New products with no reviews are invisible to all systems except Content-Based.\n\n- **Hybrid weights not optimized** ‚Äî SVD=0.40, CF=0.25, CB=0.20, Pop=0.15 were chosen by reasoning.\n  Grid search or learned meta-weights would improve both accuracy and fairness.\n\n---\n\n\n## üèÜ The Most Important Result\n\n> **User-Based CF is simultaneously the most accurate system (HR@10 = 0.026)\n> and the second most fair (Bias = 110.3, Long Tail = 8.3%).**\n>\n> Accuracy and fairness do not have to conflict.\n> The right algorithm, on the right data, honestly evaluated,\n> can serve users and small vendors at the same time.\n\n---\n\n*If this notebook was useful, an upvote helps others find it.\nQuestions, corrections, and improvements welcome in the comments.*\n\n---\n**Stack:** NumPy 2.0.2 | SciPy 1.16.3 | scikit-learn 1.6.1 | pandas 2.3.3 | matplotlib\n**Dataset:** Amazon Fine Food Reviews (SNAP, Kaggle) ","metadata":{}}]}